{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the only one column with integer values (it looks like clusters), so we divide the dataset by this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tX[:,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of clusters are:  (99913, 30) (77544, 30) (50379, 30) (22164, 30)\n"
     ]
    }
   ],
   "source": [
    "tX0=np.copy(tX[tX[:,22]==0,:])\n",
    "ids0=np.copy(ids[tX[:,22]==0])\n",
    "y0 = np.copy(y[tX[:,22]==0])\n",
    "\n",
    "tX1=np.copy(tX[tX[:,22]==1,:])\n",
    "ids1=np.copy(ids[tX[:,22]==1])\n",
    "y1 = np.copy(y[tX[:,22]==1])\n",
    "\n",
    "tX2=np.copy(tX[tX[:,22]==2,:])\n",
    "ids2=np.copy(ids[tX[:,22]==2])\n",
    "y2 =  np.copy(y[tX[:,22]==2])\n",
    "\n",
    "tX3=np.copy(tX[tX[:,22]==3,:])\n",
    "ids3=np.copy(ids[tX[:,22]==3])\n",
    "y3 =  np.copy(y[tX[:,22]==3])\n",
    "\n",
    "print('Shapes of clusters are: ',tX0.shape,tX1.shape,tX2.shape,tX3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting columns with clusters\n",
    "tX0=np.copy(np.delete(tX0,(22),axis=1))\n",
    "tX1=np.copy(np.delete(tX1,(22),axis=1))\n",
    "tX2=np.copy(np.delete(tX2,(22),axis=1))\n",
    "tX3=np.copy(np.delete(tX3,(22),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of NaNs in 0 cluster: \n",
      " [26123     0     0     0 99913 99913 99913     0     0     0     0     0\n",
      " 99913     0     0     0     0     0     0     0     0     0 99913 99913\n",
      " 99913 99913 99913 99913     0]\n",
      "Amount of NaNs in 1 cluster: \n",
      " [ 7562     0     0     0 77544 77544 77544     0     0     0     0     0\n",
      " 77544     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0 77544 77544 77544     0]\n",
      "Amount of NaNs in 2 cluster: \n",
      " [2952    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "Amount of NaNs in 3 cluster: \n",
      " [1477    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "#Counting NaNs in columns in each cluster\n",
    "print('Amount of NaNs in 0 cluster: \\n',np.count_nonzero(tX0==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 1 cluster: \\n',np.count_nonzero(tX1==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 2 cluster: \\n',np.count_nonzero(tX2==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 3 cluster: \\n',np.count_nonzero(tX3==-999.0, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in clusters 0 and 1 some columns consist only of NaNs That's why we will delete these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting columns where all values are same\n",
    "tX0cl=np.copy(tX0[:,np.invert(np.all(tX0 == tX0[0,:], axis = 0))])\n",
    "tX1cl=np.copy(tX1[:,np.invert(np.all(tX1 == tX1[0,:], axis = 0))])\n",
    "tX2cl=np.copy(tX2[:,np.invert(np.all(tX2 == tX2[0,:], axis = 0))])\n",
    "tX3cl=np.copy(tX3[:,np.invert(np.all(tX3 == tX3[0,:], axis = 0))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have cleaned all columns except the first one. We have different ways to work with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Delete rows with NaNs in the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting rows with NaNs\n",
    "ids0d=np.copy(ids0[tX0[:,0]!=-999.0])\n",
    "y0d = np.copy(y0[tX0[:,0]!=-999.0])\n",
    "tX0=np.copy(tX0[tX0[:,0]!=-999.0,:])\n",
    "\n",
    "ids1d=np.copy(ids1[tX1[:,0]!=-999.0])\n",
    "y1d = np.copy(y1[tX1[:,0]!=-999.0])\n",
    "tX1=np.copy(tX1[tX1[:,0]!=-999.0,:])\n",
    "\n",
    "ids2d=np.copy(ids2[tX2[:,0]!=-999.0])\n",
    "y2d = np.copy(y2[tX2[:,0]!=-999.0])\n",
    "tX2=np.copy(tX2[tX2[:,0]!=-999.0,:])\n",
    "\n",
    "ids3d=np.copy(ids3[tX3[:,0]!=-999.0])\n",
    "y3d = np.copy(y3[tX3[:,0]!=-999.0])\n",
    "tX3=np.copy(tX3[tX3[:,0]!=-999.0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. See the correlation between regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.9428649994377921\n",
      "Pearson correlation between column 1 and 7 : 0.7025329814432556\n",
      "Pearson correlation between column 3 and 5 : 0.9999999999991431\n",
      "Pearson correlation between column 6 and 9 : 0.7921536842510372\n",
      "Pearson correlation between column 6 and 12 : 0.7801737531532945\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX0\n",
    "for i in range(len(tX0[0,:])):\n",
    "    for j in range(i+1,len(tX0[0,:])):\n",
    "        corr = np.corrcoef(tX0[:,i],tX0[:,j])\n",
    "        if np.abs(np.corrcoef(tX0[:,i],tX0[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it makes sense to drop columns 0,3,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX0=tX0[:,[i for i in range(tX0.shape[1]) if i not in [0,3,6]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.9203955805006697\n",
      "Pearson correlation between column 3 and 6 : 0.880515673723393\n",
      "Pearson correlation between column 3 and 15 : 0.7067535021056447\n",
      "Pearson correlation between column 3 and 17 : 0.719783491337637\n",
      "Pearson correlation between column 3 and 18 : 0.9473952774313399\n",
      "Pearson correlation between column 3 and 21 : 0.9473952764775125\n",
      "Pearson correlation between column 6 and 17 : 0.7682569234879949\n",
      "Pearson correlation between column 6 and 18 : 0.9128480282626462\n",
      "Pearson correlation between column 6 and 21 : 0.9128480302418414\n",
      "Pearson correlation between column 7 and 12 : 0.7106467382693462\n",
      "Pearson correlation between column 17 and 18 : 0.721078263561643\n",
      "Pearson correlation between column 17 and 21 : 0.7210782626271482\n",
      "Pearson correlation between column 18 and 21 : 0.9999999999990334\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX1\n",
    "for i in range(len(tX1[0,:])):\n",
    "    for j in range(i+1,len(tX1[0,:])):\n",
    "        corr = np.corrcoef(tX1[:,i],tX1[:,j])\n",
    "        if np.abs(np.corrcoef(tX1[:,i],tX1[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop columns 0,3,6,21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX1=tX1[:,[i for i in range(tX1.shape[1]) if i not in [0,3,6,21]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8882697562122746\n",
      "Pearson correlation between column 3 and 9 : 0.8100583553617612\n",
      "Pearson correlation between column 3 and 19 : 0.7727347641970795\n",
      "Pearson correlation between column 3 and 21 : 0.7106329242566304\n",
      "Pearson correlation between column 3 and 22 : 0.8064082142710389\n",
      "Pearson correlation between column 3 and 28 : 0.7651054026528024\n",
      "Pearson correlation between column 4 and 5 : 0.8102770843828327\n",
      "Pearson correlation between column 4 and 6 : -0.8533511261674986\n",
      "Pearson correlation between column 5 and 6 : -0.7806386730593765\n",
      "Pearson correlation between column 9 and 21 : 0.8630680375230015\n",
      "Pearson correlation between column 9 and 22 : 0.9212004546370948\n",
      "Pearson correlation between column 9 and 28 : 0.9417446400854351\n",
      "Pearson correlation between column 10 and 16 : 0.7427175522749608\n",
      "Pearson correlation between column 21 and 22 : 0.8087380200255875\n",
      "Pearson correlation between column 21 and 28 : 0.8269810542575168\n",
      "Pearson correlation between column 22 and 28 : 0.9584434603145308\n",
      "Pearson correlation between column 25 and 28 : 0.7179977621623423\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX2\n",
    "for i in range(len(tX2[0,:])):\n",
    "    for j in range(i+1,len(tX2[0,:])):\n",
    "        corr = np.corrcoef(tX2[:,i],tX2[:,j])\n",
    "        if np.abs(np.corrcoef(tX2[:,i],tX2[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,3,4,9,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX2=tX2[:,[i for i in range(tX2.shape[1]) if i not in [0,3,4,9,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8939715045042343\n",
      "Pearson correlation between column 3 and 19 : 0.7852631621337898\n",
      "Pearson correlation between column 4 and 5 : 0.758652958037284\n",
      "Pearson correlation between column 4 and 6 : -0.7768024192719148\n",
      "Pearson correlation between column 9 and 21 : 0.9209360084498386\n",
      "Pearson correlation between column 9 and 22 : 0.8823253774045843\n",
      "Pearson correlation between column 9 and 25 : 0.7709015614616341\n",
      "Pearson correlation between column 9 and 28 : 0.957878606826293\n",
      "Pearson correlation between column 10 and 16 : 0.7636779915431535\n",
      "Pearson correlation between column 21 and 22 : 0.817603943920867\n",
      "Pearson correlation between column 21 and 25 : 0.7259662973246287\n",
      "Pearson correlation between column 21 and 28 : 0.8968125698294412\n",
      "Pearson correlation between column 22 and 28 : 0.8874163028182186\n",
      "Pearson correlation between column 25 and 28 : 0.83669471775916\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX3\n",
    "for i in range(len(tX3[0,:])):\n",
    "    for j in range(i+1,len(tX3[0,:])):\n",
    "        corr = np.corrcoef(tX3[:,i],tX3[:,j])\n",
    "        if np.abs(np.corrcoef(tX3[:,i],tX3[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,9,21,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX3=tX3[:,[i for i in range(tX3.shape[1]) if i not in [0,9,21,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Check correlation between regressors and the result value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_11=standartize(tX0)\n",
    "tX1_11=standartize(tX1)\n",
    "tX2_11=standartize(tX2)\n",
    "tX3_11=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_11..tX3_11 to y0d...y3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-2.06880312e-01, -4.58597236e-01,  3.52838215e-01,  3.23421071e-02,\n",
       "         -1.58959755e-01, -1.15919349e-02,  2.81667771e-01,  9.47439000e-05,\n",
       "          4.26477717e-04,  1.65336871e-01,  4.68799168e-03,  6.38468391e-04,\n",
       "         -2.37790473e-02, -6.95609668e-03,  3.95805980e-02]),\n",
       "  array([-2.06301728e-01, -2.50211556e-01,  2.76199667e-01,  3.96837306e-02,\n",
       "         -1.91784626e-01,  1.18899478e-01,  1.77286403e-01,  3.66955178e-03,\n",
       "         -6.06497299e-03,  2.48889753e-01, -8.70445912e-03, -8.68657328e-07,\n",
       "          4.95246061e-02,  4.34505291e-03, -2.81561347e-02,  5.48007959e-02,\n",
       "         -5.30974371e-04, -2.24981539e-03]),\n",
       "  array([-0.16371398, -0.15131667,  0.20788056,  0.00735713,  0.22116094,\n",
       "         -0.0555752 , -0.15740836,  0.13985692,  0.18505956,  0.21305616,\n",
       "         -0.00308169,  0.00068213,  0.2712701 , -0.00727277,  0.00293023,\n",
       "          0.15992448,  0.0052238 , -0.11861323,  0.00838199,  0.00867409,\n",
       "          0.0940448 ,  0.0048366 , -0.00577548, -0.1505115 ]),\n",
       "  array([-1.03408034e-01, -1.38354123e-01,  2.54979444e-01, -1.10458663e-01,\n",
       "          2.14256921e-01,  2.75855932e-02,  2.33258219e-01,  1.52286487e-02,\n",
       "         -1.74817676e-01,  7.64653969e-02,  1.02163275e-01,  7.07684288e-02,\n",
       "         -8.72203636e-03, -2.82541050e-04,  1.61474625e-01, -4.10706389e-03,\n",
       "          3.86116034e-03,  1.60786541e-02,  5.90055502e-03, -2.42244334e-03,\n",
       "         -1.32060349e-03,  3.48126383e-02,  8.27382761e-03,  1.36851344e-03,\n",
       "         -2.92132137e-01])],\n",
       " [0.8655253268669649,\n",
       "  0.9146404549646151,\n",
       "  0.8531872782441865,\n",
       "  0.9358064809053154])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [y0d, y1d, y2d, y3d]\n",
    "tXst = [tX0_11, tX1_11, tX2_11, tX3_11]\n",
    "def LinearRegressionSubmission(y, tXst):\n",
    "    w_list = []\n",
    "    rmse_list = []\n",
    "    for i in range(len(y)):\n",
    "        w, mse = least_squares(y[i], tXst[i])\n",
    "        w_list.append(w)\n",
    "        rmse_list.append(np.sqrt(2*mse))\n",
    "    return w_list, rmse_list\n",
    "LinearRegressionSubmission(y, tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear regression with polynomial expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(y, tXst):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "    \n",
    "    # define the structure of the figure\n",
    "    num_row = 2\n",
    "    num_col = 2\n",
    "    f, axs = plt.subplots(num_row, num_col)\n",
    "    mean_rmse = []\n",
    "    rmse_list = []\n",
    "    min_error = 1000\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        w0, mse0 = least_squares(y[0], build_poly(tXst[0],degree))\n",
    "        w1, mse1 = least_squares(y[1], build_poly(tXst[1],degree))\n",
    "        w2, mse2 = least_squares(y[2], build_poly(tXst[2],degree))\n",
    "        w3, mse3 = least_squares(y[3], build_poly(tXst[3],degree))\n",
    "        err= np.array([mse0,mse1,mse2,mse3])\n",
    "        rmse_list = []\n",
    "        for i in range(len(err)):\n",
    "            rmse_list.append(np.sqrt(2*err[i]))\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse_list))\n",
    "        \n",
    "        #to store the weights with the minimum training error\n",
    "        if np.mean(rmse_list) < min_error:\n",
    "            min_error = np.mean(rmse_list)\n",
    "            w0f,w1f,w2f,w3f = w0,w1,w2,w3\n",
    "\n",
    "        mean_rmse.append(np.mean(rmse_list))\n",
    "        # plot fit\n",
    "        #plot_fitted_curve(y, x, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(\"visualize_polynomial_regression\")\n",
    "    #plt.show()\n",
    "    print(\"Best degree: {degree}, training error: {err}\".format(degree=mean_rmse.index(min(mean_rmse)),err=np.min(mean_rmse)))\n",
    "    return degree, np.min(mean_rmse),[w0f,w1f,w2f,w3f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=[0.7910167024107171, 0.8857077259821294, 0.8506684510165388, 0.8640785210235167]\n",
      "Processing 2th experiment, degree=3, rmse=[0.7620364562247038, 0.8351179266767852, 0.7981937795351055, 0.8093542931845197]\n",
      "Processing 3th experiment, degree=7, rmse=[0.7506363097771088, 0.8189092402139859, 0.7709679140699045, 0.794044608684681]\n",
      "Processing 4th experiment, degree=12, rmse=[0.7467773789812082, 0.8005811377963961, 0.7551286163235877, 0.7694411157031958]\n",
      "Best degree: 3, training error: 0.767982062201097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-f4b505f0879c>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prevision0=(np.array([ids[0],ypred_list[0]]).T)\n",
      "<ipython-input-23-f4b505f0879c>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prevision1=(np.array([ids[1],ypred_list[1]]).T)\n",
      "<ipython-input-23-f4b505f0879c>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prevision2=(np.array([ids[2],ypred_list[2]]).T)\n",
      "<ipython-input-23-f4b505f0879c>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prevision3=(np.array([ids[3],ypred_list[3]]).T)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f4b505f0879c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprevision\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "degree,_,w = polynomial_regression(y,tXst)\n",
    "ypred_list = []\n",
    "rmse_list = []\n",
    "sub = []\n",
    "for i in range(4):\n",
    "    ypred = predict_labels(w[i], build_poly(tXst[i],degree))\n",
    "    ypred_list.append(ypred)\n",
    "\n",
    "#creating submission format\n",
    "prevision0=(np.array([ids[0],ypred_list[0]]).T)\n",
    "prevision1=(np.array([ids[1],ypred_list[1]]).T)\n",
    "prevision2=(np.array([ids[2],ypred_list[2]]).T)\n",
    "prevision3=(np.array([ids[3],ypred_list[3]]).T)\n",
    "prevision = np.concatenate((prevision0,prevision1,prevision2,prevision3), axis=0)\n",
    "\n",
    "\n",
    "submission = sorted(prevision, key=prevision[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-2.06571116e-01, -4.57691452e-01,  3.52283673e-01,  3.24509530e-02,\n",
       "         -1.55890631e-01, -1.15167705e-02,  2.83973209e-01,  8.44284667e-05,\n",
       "          4.19370971e-04,  1.62432538e-01,  4.68820781e-03,  6.43522838e-04,\n",
       "         -2.42093647e-02, -6.95692564e-03,  3.94856175e-02]),\n",
       "  array([-2.06545201e-01, -2.49239422e-01,  2.75249830e-01,  3.96264850e-02,\n",
       "         -1.88972644e-01,  1.19069861e-01,  1.78745359e-01,  3.66467420e-03,\n",
       "         -6.07237433e-03,  2.46473141e-01, -8.70475601e-03, -9.40519849e-06,\n",
       "          5.00612543e-02,  4.34215415e-03, -2.80314192e-02,  5.37826620e-02,\n",
       "         -5.37149489e-04, -2.25602678e-03]),\n",
       "  array([-0.16366787, -0.15035485,  0.20767427,  0.00718797,  0.22032419,\n",
       "         -0.05549441, -0.1557675 ,  0.13984364,  0.18510653,  0.21339784,\n",
       "         -0.00308145,  0.00067321,  0.26934055, -0.00727725,  0.00293162,\n",
       "          0.15962375,  0.00522433, -0.11910576,  0.00837654,  0.00867141,\n",
       "          0.09369378,  0.00483234, -0.00577308, -0.14945956]),\n",
       "  array([-1.08371191e-01, -1.35207310e-01,  2.21061208e-01, -1.09201404e-01,\n",
       "          2.12910840e-01,  2.74365532e-02,  2.23203887e-01,  1.48695329e-02,\n",
       "         -1.67880508e-01,  7.97805614e-02,  1.01888570e-01,  8.23417487e-02,\n",
       "         -8.87124709e-03, -2.77498122e-04,  1.66995523e-01, -4.08077167e-03,\n",
       "          4.00495658e-03,  3.53828371e-02,  5.98199139e-03, -2.28367501e-03,\n",
       "         -1.16214584e-03,  3.30540654e-02,  8.30002415e-03,  1.46978760e-03,\n",
       "         -2.88576206e-01])],\n",
       " [0.3745677365726327,\n",
       "  0.4182844453765566,\n",
       "  0.3639647765547564,\n",
       "  0.4378991373830783])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GradientDescentSubmission(y, tXst):\n",
    "  # Define the parameters of the algorithm.\n",
    "    max_iters = 100\n",
    "    gamma = 0.5\n",
    "  # Initialization\n",
    "    \n",
    "    w_list = []\n",
    "    loss_list = []\n",
    "  # Start GD\n",
    "    for i in range(4):\n",
    "        w_initial = np.array([0 for i in range(tXst[i].shape[1])])\n",
    "        gradient_loss, gradient_w = gradient_descent(y[i], tXst[i], w_initial, max_iters, gamma)\n",
    "        w_list.append(gradient_w)\n",
    "        loss_list.append(gradient_loss)\n",
    "    return w_list, loss_list\n",
    "GradientDescentSubmission(y,tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-0.02719863, -0.00297574,  0.01284063, -0.01001323, -0.02731909,\n",
       "          0.00103735,  0.02411653, -0.0197074 ,  0.0066014 , -0.01168469,\n",
       "         -0.00705803, -0.00097549, -0.00797202, -0.00085984, -0.00737934]),\n",
       "  array([-0.01277393, -0.0215578 , -0.00166476,  0.01207366, -0.01809813,\n",
       "          0.00854048,  0.0098442 ,  0.00085613,  0.00142487, -0.0114531 ,\n",
       "          0.00940709, -0.0045363 ,  0.00766401,  0.0022265 ,  0.00252269,\n",
       "          0.00426243,  0.00303347, -0.01523766]),\n",
       "  array([-0.01510539,  0.00331314,  0.02967192, -0.02193179,  0.00126006,\n",
       "         -0.01268202, -0.00045906,  0.01861773,  0.03498346,  0.00951184,\n",
       "         -0.01901221, -0.00679157,  0.00751485, -0.01456917, -0.0072967 ,\n",
       "          0.0129295 , -0.01703568, -0.00254135,  0.00318531,  0.00621343,\n",
       "          0.01303673, -0.00915203,  0.00547727,  0.01437739]),\n",
       "  array([-7.38686124e-03,  1.10789374e-02, -1.14468174e-03,  7.25467868e-03,\n",
       "         -1.77912862e-03,  2.83574044e-03,  8.72306133e-03, -5.14986003e-04,\n",
       "         -6.74995447e-03, -2.97105264e-03,  1.26211184e-02,  3.95464238e-03,\n",
       "         -4.66809139e-03,  2.38717493e-03, -2.49872690e-03, -8.34121726e-05,\n",
       "          5.06232431e-03,  2.47642528e-03,  1.01173100e-02, -1.80218069e-02,\n",
       "          4.23341795e-03, -2.29012326e-02,  1.23916935e-02, -2.11577460e-04,\n",
       "         -2.98187041e-02])],\n",
       " [0.47036904858651213,\n",
       "  0.4866679762106996,\n",
       "  0.45914554171817495,\n",
       "  0.4955387756263424])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def StochasticGradientDescent(y, tXst):\n",
    "  # Define the parameters of the algorithm.\n",
    "    max_iters = 100\n",
    "    gamma = 0.001\n",
    "    batch_size = 1\n",
    "  # Initialization\n",
    "    w_list = []\n",
    "    loss_list = []\n",
    "  # Start SGD.\n",
    "    for i in range(4):\n",
    "        w_initial = np.array([0 for i in range(tXst[i].shape[1])])\n",
    "        sgd_losses, sgd_w = stochastic_gradient_descent(y[i], tXst[i], w_initial, batch_size, max_iters, gamma)\n",
    "        w_list.append(sgd_w)\n",
    "        loss_list.append(sgd_losses)\n",
    "    return w_list, loss_list\n",
    "StochasticGradientDescent(y, tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for 0 subset are: 1 0.01  with error:  0.7929677144488401\n",
      "Best parameters for 1 subset are: 2 0.01  with error:  0.8450561850194014\n",
      "Best parameters for 2 subset are: 2 0.01  with error:  0.8173424201564052\n",
      "Best parameters for 3 subset are: 2 0.01  with error:  0.8320016918872625\n",
      "[[0.93630976 0.93645421 0.93732362 0.94137645 0.95317962 0.97125258\n",
      "  0.98617216 0.99424679]\n",
      " [0.79296771 0.7959668  0.80461279 0.82188364 0.85171029 0.89660549\n",
      "  0.94290872 0.97407404]\n",
      " [2.63784812 2.50596282 2.11248506 1.4968839  0.92503176 0.94594117\n",
      "  1.17291633 1.16651706]] [[0.97364297 0.97370116 0.9740531  0.97569909 0.98052021 0.98797365\n",
      "  0.99418946 0.99757674]\n",
      " [0.88634788 0.88770695 0.89194932 0.90119727 0.91843812 0.94433812\n",
      "  0.96988659 0.98647535]\n",
      " [0.84505619 0.84631685 0.84932718 0.85628768 0.8726649  0.90067554\n",
      "  0.93105463 0.95601466]] [[0.99785752 0.99786204 0.99788998 0.9980218  0.99841024 0.99901492\n",
      "  0.99952264 0.9998006 ]\n",
      " [0.85156878 0.85317203 0.85792353 0.86728612 0.88394989 0.91290307\n",
      "  0.94846495 0.97555061]\n",
      " [0.81734242 0.81842084 0.82230268 0.83287073 0.85398572 0.88540182\n",
      "  0.92024966 0.95092966]] [[0.93321668 0.93336927 0.93428432 0.93854342 0.95093514 0.96988973\n",
      "  0.98552227 0.99397763]\n",
      " [0.86587244 0.86764829 0.87254226 0.88319215 0.90506429 0.93751498\n",
      "  0.96714593 0.98541129]\n",
      " [0.83200169 0.83433777 0.83868516 0.8473625  0.86204602 0.88072703\n",
      "  0.90335707 0.93007632]]\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-2, 1, 8)\n",
    "    # split data in k fold\n",
    "    num_degrees=3\n",
    "    # define lists to store the loss of training data and test data\n",
    "    #matrices for storing lists (for 4 subsets) of errors\n",
    "    #matrix_te=np.zeros(shape=(num_degrees,len(lambdas)),dtype=list)\n",
    "    #matrix_tr=np.zeros(shape=(num_degrees,len(lambdas)),dtype=list)\n",
    "    matrix_te0=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr0=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te1=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr1=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te2=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr2=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te3=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr3=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    for degree in np.arange(num_degrees):\n",
    "        for ind_lmbd,lambda_ in enumerate(lambdas):\n",
    "            #losses_te=[]\n",
    "            #losses_tr=[]\n",
    "            for i in range(0,4):\n",
    "                k_indices,indices = build_k_indices(y[i], k_fold, seed)\n",
    "                loss_tr=0\n",
    "                loss_te=0\n",
    "                for k in range(k_fold):\n",
    "                    #print([degree,ind_lmbd,i,k])\n",
    "                    l_tr, l_te,_ = cross_validation_ridge(y[i], tXst[i], k_indices, k, lambda_, degree)\n",
    "                    loss_tr+=l_tr\n",
    "                    loss_te+=l_te\n",
    "                if i==0:\n",
    "                    matrix_te0[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr0[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==1:\n",
    "                    matrix_te1[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr1[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==2:\n",
    "                    matrix_te2[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr2[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==3:\n",
    "                    matrix_te3[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr3[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                #losses_te.append(np.mean(loss_te))\n",
    "                #losses_tr.append(np.mean(loss_tr))\n",
    "            #matrix_tr[degree][ind_lmbd]= losses_tr\n",
    "            #matrix_te[degree][ind_lmbd]=losses_te\n",
    "    \n",
    "   \n",
    "    \n",
    "    #get the best degree lambda couple  \n",
    "    result = np.where(matrix_te0 == np.amin(matrix_te0))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 0 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te0[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te1 == np.amin(matrix_te1))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 1 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te1[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te2 == np.amin(matrix_te2))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 2 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te2[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te3 == np.amin(matrix_te3))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 3 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te3[best_degree][best_lambda])\n",
    "    \n",
    "    print(matrix_te0,matrix_te1,matrix_te2,matrix_te3)\n",
    "    #show cross validation for each degree\n",
    "    \"\"\"\n",
    "    ind_row = 0\n",
    "    for row in matrix_te:\n",
    "        degree = ind_row\n",
    "        lambdas = list(range(len(row)))\n",
    "        rmse_te = row\n",
    "        rmse_tr = matrix_tr[ind_row, :]\n",
    "        cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "        ind_row += 1\n",
    "    plt.show()\n",
    "\n",
    "    return best_degree, best_lambda\n",
    "    \"\"\"\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Don't see the correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_12=standartize(tX0cl)\n",
    "tX1_12=standartize(tX1cl)\n",
    "tX2_12=standartize(tX2cl)\n",
    "tX3_12=standartize(tX3cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_12..tX3_12 to y0d...y3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Substitute rows with NaNs in the first column by the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean for 1st column without -999.0 values\n",
    "mean0=tX0[tX0[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean1=tX1[tX1[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean2=tX2[tX2[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean3=tX3[tX3[:,0]!=-999.0].mean(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX0[tX0[:,0]!=-999.0].mean(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX0=np.copy(np.where(tX0==-999.0,mean0,tX0))\n",
    "tX1=np.copy(np.where(tX1==-999.0,mean1,tX1))\n",
    "tX2=np.copy(np.where(tX2==-999.0,mean2,tX2))\n",
    "tX3=np.copy(np.where(tX3==-999.0,mean3,tX3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. See the correlation between regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.7488162009965309\n",
      "Pearson correlation between column 3 and 5 : 0.9999999999988465\n",
      "Pearson correlation between column 6 and 9 : 0.8002688533328596\n",
      "Pearson correlation between column 6 and 12 : 0.7797439568172713\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX0\n",
    "for i in range(len(tX0[0,:])):\n",
    "    for j in range(i+1,len(tX0[0,:])):\n",
    "        corr = np.corrcoef(tX0[:,i],tX0[:,j])\n",
    "        if np.abs(np.corrcoef(tX0[:,i],tX0[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it makes sense to drop columns 0,3,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX0=tX0[:,[i for i in range(tX0.shape[1]) if i not in [0,3,6]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8445009835185863\n",
      "Pearson correlation between column 3 and 6 : 0.8632110677076686\n",
      "Pearson correlation between column 3 and 17 : 0.7125818159025532\n",
      "Pearson correlation between column 3 and 18 : 0.9367590332144766\n",
      "Pearson correlation between column 3 and 21 : 0.9367590311733165\n",
      "Pearson correlation between column 6 and 17 : 0.757990128208487\n",
      "Pearson correlation between column 6 and 18 : 0.9051662981927852\n",
      "Pearson correlation between column 6 and 21 : 0.905166300382275\n",
      "Pearson correlation between column 7 and 12 : 0.709867998636906\n",
      "Pearson correlation between column 17 and 18 : 0.7068771933564622\n",
      "Pearson correlation between column 17 and 21 : 0.7068771905369512\n",
      "Pearson correlation between column 18 and 21 : 0.999999999999017\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX1\n",
    "for i in range(len(tX1[0,:])):\n",
    "    for j in range(i+1,len(tX1[0,:])):\n",
    "        corr = np.corrcoef(tX1[:,i],tX1[:,j])\n",
    "        if np.abs(np.corrcoef(tX1[:,i],tX1[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop columns 0,3,6,21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX1=tX1[:,[i for i in range(tX1.shape[1]) if i not in [0,3,6,21]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8228939521810881\n",
      "Pearson correlation between column 3 and 9 : 0.800559238241552\n",
      "Pearson correlation between column 3 and 19 : 0.7285433946032545\n",
      "Pearson correlation between column 3 and 21 : 0.7069249354905159\n",
      "Pearson correlation between column 3 and 22 : 0.7995787774292449\n",
      "Pearson correlation between column 3 and 28 : 0.7584645222360212\n",
      "Pearson correlation between column 4 and 5 : 0.8098715851425442\n",
      "Pearson correlation between column 4 and 6 : -0.8501315483608742\n",
      "Pearson correlation between column 5 and 6 : -0.7780946604694609\n",
      "Pearson correlation between column 9 and 21 : 0.8523147287795626\n",
      "Pearson correlation between column 9 and 22 : 0.9191164357409889\n",
      "Pearson correlation between column 9 and 28 : 0.9397870414625126\n",
      "Pearson correlation between column 10 and 16 : 0.7438565749365871\n",
      "Pearson correlation between column 21 and 22 : 0.7970772581576845\n",
      "Pearson correlation between column 21 and 28 : 0.8147451242277628\n",
      "Pearson correlation between column 22 and 28 : 0.9584459537718197\n",
      "Pearson correlation between column 25 and 28 : 0.720740253151538\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX2\n",
    "for i in range(len(tX2[0,:])):\n",
    "    for j in range(i+1,len(tX2[0,:])):\n",
    "        corr = np.corrcoef(tX2[:,i],tX2[:,j])\n",
    "        if np.abs(np.corrcoef(tX2[:,i],tX2[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,3,4,9,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX2=tX2[:,[i for i in range(tX2.shape[1]) if i not in [0,3,4,9,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8172288262177909\n",
      "Pearson correlation between column 3 and 19 : 0.7502599406025436\n",
      "Pearson correlation between column 4 and 5 : 0.758966330984028\n",
      "Pearson correlation between column 4 and 6 : -0.7735086978837818\n",
      "Pearson correlation between column 9 and 21 : 0.9160946827155181\n",
      "Pearson correlation between column 9 and 22 : 0.8813251894172176\n",
      "Pearson correlation between column 9 and 25 : 0.7708057103889608\n",
      "Pearson correlation between column 9 and 28 : 0.9569498612168339\n",
      "Pearson correlation between column 10 and 16 : 0.7683893227784884\n",
      "Pearson correlation between column 21 and 22 : 0.8132272408011297\n",
      "Pearson correlation between column 21 and 25 : 0.7219329222297065\n",
      "Pearson correlation between column 21 and 28 : 0.892063808128829\n",
      "Pearson correlation between column 22 and 28 : 0.887903038587193\n",
      "Pearson correlation between column 25 and 28 : 0.8365333266992916\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX3\n",
    "for i in range(len(tX3[0,:])):\n",
    "    for j in range(i+1,len(tX3[0,:])):\n",
    "        corr = np.corrcoef(tX3[:,i],tX3[:,j])\n",
    "        if np.abs(np.corrcoef(tX3[:,i],tX3[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,9,21,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX3=tX3[:,[i for i in range(tX3.shape[1]) if i not in [0,9,21,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Check correlation between regressors and the result value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_21=standartize(tX0)\n",
    "tX1_21=standartize(tX1)\n",
    "tX2_21=standartize(tX2)\n",
    "tX3_21=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_21..tX3_21 to y0...y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Don't the correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)\n",
    "\n",
    "#Replace NaNs with median\n",
    "tX0=np.copy(np.where(tX0==-999.0,mean0,tX0))\n",
    "tX1=np.copy(np.where(tX1==-999.0,mean1,tX1))\n",
    "tX2=np.copy(np.where(tX2==-999.0,mean2,tX2))\n",
    "tX3=np.copy(np.where(tX3==-999.0,mean3,tX3))\n",
    "\n",
    "#Data standartization\n",
    "tX0_22=standartize(tX0)\n",
    "tX1_22=standartize(tX1)\n",
    "tX2_22=standartize(tX2)\n",
    "tX3_22=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_22..tX3_22 to y0...y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
