{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the only one column with integer values (it looks like clusters), so we divide the dataset by this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tX[:,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of clusters are:  (99913, 30) (77544, 30) (50379, 30) (22164, 30)\n"
     ]
    }
   ],
   "source": [
    "tX0=np.copy(tX[tX[:,22]==0,:])\n",
    "ids0=np.copy(ids[tX[:,22]==0])\n",
    "y0 = np.copy(y[tX[:,22]==0])\n",
    "\n",
    "tX1=np.copy(tX[tX[:,22]==1,:])\n",
    "ids1=np.copy(ids[tX[:,22]==1])\n",
    "y1 = np.copy(y[tX[:,22]==1])\n",
    "\n",
    "tX2=np.copy(tX[tX[:,22]==2,:])\n",
    "ids2=np.copy(ids[tX[:,22]==2])\n",
    "y2 =  np.copy(y[tX[:,22]==2])\n",
    "\n",
    "tX3=np.copy(tX[tX[:,22]==3,:])\n",
    "ids3=np.copy(ids[tX[:,22]==3])\n",
    "y3 =  np.copy(y[tX[:,22]==3])\n",
    "\n",
    "print('Shapes of clusters are: ',tX0.shape,tX1.shape,tX2.shape,tX3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting columns with clusters\n",
    "tX0=np.copy(np.delete(tX0,(22),axis=1))\n",
    "tX1=np.copy(np.delete(tX1,(22),axis=1))\n",
    "tX2=np.copy(np.delete(tX2,(22),axis=1))\n",
    "tX3=np.copy(np.delete(tX3,(22),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of NaNs in 0 cluster: \n",
      " [26123     0     0     0 99913 99913 99913     0     0     0     0     0\n",
      " 99913     0     0     0     0     0     0     0     0     0 99913 99913\n",
      " 99913 99913 99913 99913     0]\n",
      "Amount of NaNs in 1 cluster: \n",
      " [ 7562     0     0     0 77544 77544 77544     0     0     0     0     0\n",
      " 77544     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0 77544 77544 77544     0]\n",
      "Amount of NaNs in 2 cluster: \n",
      " [2952    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "Amount of NaNs in 3 cluster: \n",
      " [1477    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "#Counting NaNs in columns in each cluster\n",
    "print('Amount of NaNs in 0 cluster: \\n',np.count_nonzero(tX0==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 1 cluster: \\n',np.count_nonzero(tX1==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 2 cluster: \\n',np.count_nonzero(tX2==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 3 cluster: \\n',np.count_nonzero(tX3==-999.0, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in clusters 0 and 1 some columns consist only of NaNs That's why we will delete these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting columns where all values are same\n",
    "tX0cl=np.copy(tX0[:,np.invert(np.all(tX0 == tX0[0,:], axis = 0))])\n",
    "tX1cl=np.copy(tX1[:,np.invert(np.all(tX1 == tX1[0,:], axis = 0))])\n",
    "tX2cl=np.copy(tX2[:,np.invert(np.all(tX2 == tX2[0,:], axis = 0))])\n",
    "tX3cl=np.copy(tX3[:,np.invert(np.all(tX3 == tX3[0,:], axis = 0))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have cleaned all columns except the first one. We have different ways to work with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Delete rows with NaNs in the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting rows with NaNs\n",
    "ids0d=np.copy(ids0[tX0[:,0]!=-999.0])\n",
    "y0d = np.copy(y0[tX0[:,0]!=-999.0])\n",
    "tX0=np.copy(tX0[tX0[:,0]!=-999.0,:])\n",
    "\n",
    "ids1d=np.copy(ids1[tX1[:,0]!=-999.0])\n",
    "y1d = np.copy(y1[tX1[:,0]!=-999.0])\n",
    "tX1=np.copy(tX1[tX1[:,0]!=-999.0,:])\n",
    "\n",
    "ids2d=np.copy(ids2[tX2[:,0]!=-999.0])\n",
    "y2d = np.copy(y2[tX2[:,0]!=-999.0])\n",
    "tX2=np.copy(tX2[tX2[:,0]!=-999.0,:])\n",
    "\n",
    "ids3d=np.copy(ids3[tX3[:,0]!=-999.0])\n",
    "y3d = np.copy(y3[tX3[:,0]!=-999.0])\n",
    "tX3=np.copy(tX3[tX3[:,0]!=-999.0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. See the correlation between regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.9428649994377921\n",
      "Pearson correlation between column 1 and 7 : 0.7025329814432556\n",
      "Pearson correlation between column 3 and 5 : 0.9999999999991431\n",
      "Pearson correlation between column 6 and 9 : 0.7921536842510372\n",
      "Pearson correlation between column 6 and 12 : 0.7801737531532945\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX0\n",
    "for i in range(len(tX0[0,:])):\n",
    "    for j in range(i+1,len(tX0[0,:])):\n",
    "        corr = np.corrcoef(tX0[:,i],tX0[:,j])\n",
    "        if np.abs(np.corrcoef(tX0[:,i],tX0[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it makes sense to drop columns 0,3,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX0=tX0[:,[i for i in range(tX0.shape[1]) if i not in [0,3,6]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.9203955805006697\n",
      "Pearson correlation between column 3 and 6 : 0.880515673723393\n",
      "Pearson correlation between column 3 and 15 : 0.7067535021056447\n",
      "Pearson correlation between column 3 and 17 : 0.719783491337637\n",
      "Pearson correlation between column 3 and 18 : 0.9473952774313399\n",
      "Pearson correlation between column 3 and 21 : 0.9473952764775125\n",
      "Pearson correlation between column 6 and 17 : 0.7682569234879949\n",
      "Pearson correlation between column 6 and 18 : 0.9128480282626462\n",
      "Pearson correlation between column 6 and 21 : 0.9128480302418414\n",
      "Pearson correlation between column 7 and 12 : 0.7106467382693462\n",
      "Pearson correlation between column 17 and 18 : 0.721078263561643\n",
      "Pearson correlation between column 17 and 21 : 0.7210782626271482\n",
      "Pearson correlation between column 18 and 21 : 0.9999999999990334\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX1\n",
    "for i in range(len(tX1[0,:])):\n",
    "    for j in range(i+1,len(tX1[0,:])):\n",
    "        corr = np.corrcoef(tX1[:,i],tX1[:,j])\n",
    "        if np.abs(np.corrcoef(tX1[:,i],tX1[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop columns 0,3,6,21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX1=tX1[:,[i for i in range(tX1.shape[1]) if i not in [0,3,6,21]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8882697562122746\n",
      "Pearson correlation between column 3 and 9 : 0.8100583553617612\n",
      "Pearson correlation between column 3 and 19 : 0.7727347641970795\n",
      "Pearson correlation between column 3 and 21 : 0.7106329242566304\n",
      "Pearson correlation between column 3 and 22 : 0.8064082142710389\n",
      "Pearson correlation between column 3 and 28 : 0.7651054026528024\n",
      "Pearson correlation between column 4 and 5 : 0.8102770843828327\n",
      "Pearson correlation between column 4 and 6 : -0.8533511261674986\n",
      "Pearson correlation between column 5 and 6 : -0.7806386730593765\n",
      "Pearson correlation between column 9 and 21 : 0.8630680375230015\n",
      "Pearson correlation between column 9 and 22 : 0.9212004546370948\n",
      "Pearson correlation between column 9 and 28 : 0.9417446400854351\n",
      "Pearson correlation between column 10 and 16 : 0.7427175522749608\n",
      "Pearson correlation between column 21 and 22 : 0.8087380200255875\n",
      "Pearson correlation between column 21 and 28 : 0.8269810542575168\n",
      "Pearson correlation between column 22 and 28 : 0.9584434603145308\n",
      "Pearson correlation between column 25 and 28 : 0.7179977621623423\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX2\n",
    "for i in range(len(tX2[0,:])):\n",
    "    for j in range(i+1,len(tX2[0,:])):\n",
    "        corr = np.corrcoef(tX2[:,i],tX2[:,j])\n",
    "        if np.abs(np.corrcoef(tX2[:,i],tX2[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,3,4,9,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX2=tX2[:,[i for i in range(tX2.shape[1]) if i not in [0,3,4,9,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8939715045042343\n",
      "Pearson correlation between column 3 and 19 : 0.7852631621337898\n",
      "Pearson correlation between column 4 and 5 : 0.758652958037284\n",
      "Pearson correlation between column 4 and 6 : -0.7768024192719148\n",
      "Pearson correlation between column 9 and 21 : 0.9209360084498386\n",
      "Pearson correlation between column 9 and 22 : 0.8823253774045843\n",
      "Pearson correlation between column 9 and 25 : 0.7709015614616341\n",
      "Pearson correlation between column 9 and 28 : 0.957878606826293\n",
      "Pearson correlation between column 10 and 16 : 0.7636779915431535\n",
      "Pearson correlation between column 21 and 22 : 0.817603943920867\n",
      "Pearson correlation between column 21 and 25 : 0.7259662973246287\n",
      "Pearson correlation between column 21 and 28 : 0.8968125698294412\n",
      "Pearson correlation between column 22 and 28 : 0.8874163028182186\n",
      "Pearson correlation between column 25 and 28 : 0.83669471775916\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX3\n",
    "for i in range(len(tX3[0,:])):\n",
    "    for j in range(i+1,len(tX3[0,:])):\n",
    "        corr = np.corrcoef(tX3[:,i],tX3[:,j])\n",
    "        if np.abs(np.corrcoef(tX3[:,i],tX3[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,9,21,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX3=tX3[:,[i for i in range(tX3.shape[1]) if i not in [0,9,21,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Check correlation between regressors and the result value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_11=standartize(tX0)\n",
    "tX1_11=standartize(tX1)\n",
    "tX2_11=standartize(tX2)\n",
    "tX3_11=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_11..tX3_11 to y0d...y3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-2.06880312e-01, -4.58597236e-01,  3.52838215e-01,  3.23421071e-02,\n",
       "         -1.58959755e-01, -1.15919349e-02,  2.81667771e-01,  9.47439000e-05,\n",
       "          4.26477717e-04,  1.65336871e-01,  4.68799168e-03,  6.38468391e-04,\n",
       "         -2.37790473e-02, -6.95609668e-03,  3.95805980e-02]),\n",
       "  array([-2.06301728e-01, -2.50211556e-01,  2.76199667e-01,  3.96837306e-02,\n",
       "         -1.91784626e-01,  1.18899478e-01,  1.77286403e-01,  3.66955178e-03,\n",
       "         -6.06497299e-03,  2.48889753e-01, -8.70445912e-03, -8.68657328e-07,\n",
       "          4.95246061e-02,  4.34505291e-03, -2.81561347e-02,  5.48007959e-02,\n",
       "         -5.30974371e-04, -2.24981539e-03]),\n",
       "  array([-0.16371398, -0.15131667,  0.20788056,  0.00735713,  0.22116094,\n",
       "         -0.0555752 , -0.15740836,  0.13985692,  0.18505956,  0.21305616,\n",
       "         -0.00308169,  0.00068213,  0.2712701 , -0.00727277,  0.00293023,\n",
       "          0.15992448,  0.0052238 , -0.11861323,  0.00838199,  0.00867409,\n",
       "          0.0940448 ,  0.0048366 , -0.00577548, -0.1505115 ]),\n",
       "  array([-1.03408034e-01, -1.38354123e-01,  2.54979444e-01, -1.10458663e-01,\n",
       "          2.14256921e-01,  2.75855932e-02,  2.33258219e-01,  1.52286487e-02,\n",
       "         -1.74817676e-01,  7.64653969e-02,  1.02163275e-01,  7.07684288e-02,\n",
       "         -8.72203636e-03, -2.82541050e-04,  1.61474625e-01, -4.10706389e-03,\n",
       "          3.86116034e-03,  1.60786541e-02,  5.90055502e-03, -2.42244334e-03,\n",
       "         -1.32060349e-03,  3.48126383e-02,  8.27382761e-03,  1.36851344e-03,\n",
       "         -2.92132137e-01])],\n",
       " [0.8655253268669649,\n",
       "  0.9146404549646151,\n",
       "  0.8531872782441865,\n",
       "  0.9358064809053154])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [y0d, y1d, y2d, y3d]\n",
    "tXst = [tX0_11, tX1_11, tX2_11, tX3_11]\n",
    "def LinearRegressionSubmission(y, tXst):\n",
    "    w_list = []\n",
    "    rmse_list = []\n",
    "    for i in range(len(y)):\n",
    "        w, mse = least_squares(y[i], tXst[i])\n",
    "        w_list.append(w)\n",
    "        rmse_list.append(np.sqrt(2*mse))\n",
    "    return w_list, rmse_list\n",
    "LinearRegressionSubmission(y, tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear regression with polynomial expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(y,tXst):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [i for i in range(12)]\n",
    "    mean_rmse = np.zeros((4,len(degrees)))\n",
    "    k_fold = 4\n",
    "    seed = 1\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    test = []\n",
    "    deg = []\n",
    "    minimum = []\n",
    "    for ind,degree in enumerate(degrees):\n",
    "        for i in range(0,4):\n",
    "            print(\"Subset {i}, degree {degree}\".format(i = i, degree=degree))\n",
    "            k_indices,indices = build_k_indices(y[i], k_fold,seed)\n",
    "            loss_tr=0\n",
    "            loss_te=0\n",
    "            rmse_te_tmp = []\n",
    "            for k in range(k_fold):\n",
    "                l_tr, l_te = cross_validation_leastsquares(y[i], tXst[i], k_indices, k,degree)\n",
    "                #loss_tr+=l_tr\n",
    "                loss_te+=l_te\n",
    "                #rmse_tr_tmp.append(l_te)\n",
    "            mean_rmse[i][ind]=loss_te/k_fold\n",
    "            print(mean_rmse[i][ind])\n",
    "            \n",
    "    for i in range(0,4):\n",
    "        test.append(mean_rmse[i].tolist())\n",
    "        #print(test[i])\n",
    "        deg.append(degrees[np.argmin(mean_rmse[i])])\n",
    "        minimum.append(min(test[i]))\n",
    "        print(\"Best degree for subset {i}: {degree}, test error: {err}\".format(i = i,degree=degrees[np.argmin(mean_rmse[i])],err=min(test[i])))\n",
    "    return deg , minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 0, degree 0\n",
      "0.9362851108032995\n",
      "Subset 1, degree 0\n",
      "0.9736332394593022\n",
      "Subset 2, degree 0\n",
      "0.9978568407102907\n",
      "Subset 3, degree 0\n",
      "0.9331902231837701\n",
      "Subset 0, degree 1\n",
      "0.7919284693081986\n",
      "Subset 1, degree 1\n",
      "0.8859956769469683\n",
      "Subset 2, degree 1\n",
      "0.851156681153518\n",
      "Subset 3, degree 1\n",
      "0.86537897664666\n",
      "Subset 0, degree 2\n",
      "2.564567067352238\n",
      "Subset 1, degree 2\n",
      "0.8446145338160909\n",
      "Subset 2, degree 2\n",
      "0.81718062401826\n",
      "Subset 3, degree 2\n",
      "0.8306381201178299\n",
      "Subset 0, degree 3\n",
      "81.63576812355517\n",
      "Subset 1, degree 3\n",
      "0.8421616433385937\n",
      "Subset 2, degree 3\n",
      "0.8036165091579167\n",
      "Subset 3, degree 3\n",
      "0.8199369424492217\n",
      "Subset 0, degree 4\n",
      "549.5368043380818\n",
      "Subset 1, degree 4\n",
      "0.8510783592721152\n",
      "Subset 2, degree 4\n",
      "0.8441901470491319\n",
      "Subset 3, degree 4\n",
      "0.8198653741363071\n",
      "Subset 0, degree 5\n",
      "804739.7968592497\n",
      "Subset 1, degree 5\n",
      "0.8745202202262206\n",
      "Subset 2, degree 5\n",
      "1.2959233522768474\n",
      "Subset 3, degree 5\n",
      "1.1069260678625246\n",
      "Subset 0, degree 6\n",
      "45383418.163722746\n",
      "Subset 1, degree 6\n",
      "1.0389525450967274\n",
      "Subset 2, degree 6\n",
      "2.3474091006887163\n",
      "Subset 3, degree 6\n",
      "4.648071837936476\n",
      "Subset 0, degree 7\n",
      "2564876011.9537005\n",
      "Subset 1, degree 7\n",
      "3.2254878624376015\n",
      "Subset 2, degree 7\n",
      "10.235280313954352\n",
      "Subset 3, degree 7\n",
      "37.59331163480998\n",
      "Subset 0, degree 8\n",
      "192838787699.81458\n",
      "Subset 1, degree 8\n",
      "9.901164890206404\n",
      "Subset 2, degree 8\n",
      "53.96149558399487\n",
      "Subset 3, degree 8\n",
      "122.25252166894005\n",
      "Subset 0, degree 9\n",
      "25675442945857.918\n",
      "Subset 1, degree 9\n",
      "21.893501921487456\n",
      "Subset 2, degree 9\n",
      "192.0273484543733\n",
      "Subset 3, degree 9\n",
      "891.6680904042441\n",
      "Subset 0, degree 10\n",
      "1696871196761418.5\n",
      "Subset 1, degree 10\n",
      "19.29256871761151\n",
      "Subset 2, degree 10\n",
      "1880.5687177745708\n",
      "Subset 3, degree 10\n",
      "3205.2624990014683\n",
      "Subset 0, degree 11\n",
      "1.5842749054861917e+17\n",
      "Subset 1, degree 11\n",
      "110.70706785152689\n",
      "Subset 2, degree 11\n",
      "5536.567669916678\n",
      "Subset 3, degree 11\n",
      "7253.640121878592\n",
      "Best degree for subset 0: 1, test error: 0.7919284693081986\n",
      "Best degree for subset 1: 3, test error: 0.8421616433385937\n",
      "Best degree for subset 2: 3, test error: 0.8036165091579167\n",
      "Best degree for subset 3: 4, test error: 0.8198653741363071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 3, 3, 4],\n",
       " [0.7919284693081986,\n",
       "  0.8421616433385937,\n",
       "  0.8036165091579167,\n",
       "  0.8198653741363071])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_regression(y,tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-2.06880312e-01, -4.58597236e-01,  3.52838215e-01,  3.23421071e-02,\n",
       "         -1.58959755e-01, -1.15919349e-02,  2.81667771e-01,  9.47439000e-05,\n",
       "          4.26477717e-04,  1.65336871e-01,  4.68799168e-03,  6.38468391e-04,\n",
       "         -2.37790474e-02, -6.95609668e-03,  3.95805980e-02]),\n",
       "  array([-2.06301728e-01, -2.50211556e-01,  2.76199667e-01,  3.96837306e-02,\n",
       "         -1.91784626e-01,  1.18899478e-01,  1.77286403e-01,  3.66955178e-03,\n",
       "         -6.06497299e-03,  2.48889753e-01, -8.70445912e-03, -8.68657328e-07,\n",
       "          4.95246061e-02,  4.34505291e-03, -2.81561347e-02,  5.48007959e-02,\n",
       "         -5.30974371e-04, -2.24981539e-03]),\n",
       "  array([-0.16371398, -0.15131667,  0.20788056,  0.00735713,  0.22116094,\n",
       "         -0.0555752 , -0.15740836,  0.13985692,  0.18505956,  0.21305616,\n",
       "         -0.00308169,  0.00068213,  0.2712701 , -0.00727277,  0.00293023,\n",
       "          0.15992448,  0.0052238 , -0.11861323,  0.00838199,  0.00867409,\n",
       "          0.0940448 ,  0.0048366 , -0.00577548, -0.1505115 ]),\n",
       "  array([-1.03408032e-01, -1.38354124e-01,  2.54979461e-01, -1.10458663e-01,\n",
       "          2.14256921e-01,  2.75855934e-02,  2.33258224e-01,  1.52286488e-02,\n",
       "         -1.74817678e-01,  7.64653953e-02,  1.02163275e-01,  7.07684229e-02,\n",
       "         -8.72203627e-03, -2.82541058e-04,  1.61474621e-01, -4.10706392e-03,\n",
       "          3.86116028e-03,  1.60786441e-02,  5.90055498e-03, -2.42244340e-03,\n",
       "         -1.32060356e-03,  3.48126389e-02,  8.27382760e-03,  1.36851339e-03,\n",
       "         -2.92132138e-01])],\n",
       " [0.3745670457240832,\n",
       "  0.418283580928939,\n",
       "  0.3639642658788616,\n",
       "  0.43786688485219516])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GradientDescentSubmission(y, tXst):\n",
    "  # Define the parameters of the algorithm.\n",
    "    max_iters = 1000\n",
    "    gamma = 0.5\n",
    "  # Initialization\n",
    "    \n",
    "    w_list = []\n",
    "    loss_list = []\n",
    "  # Start GD\n",
    "    for i in range(4):\n",
    "        w_initial = np.array([np.random.rand() for i in range(tXst[i].shape[1])])\n",
    "        gradient_loss, gradient_w = gradient_descent(y[i], tXst[i], w_initial, max_iters, gamma)\n",
    "        w_list.append(gradient_w)\n",
    "        loss_list.append(gradient_loss)\n",
    "    return w_list, loss_list\n",
    "GradientDescentSubmission(y,tXst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7626855144634183"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-0.03953648, -0.00835486,  0.01807197, -0.00663377, -0.02433187,\n",
       "          0.01337897,  0.01250804,  0.00348614, -0.00726161, -0.01795264,\n",
       "         -0.0077468 ,  0.01124808, -0.02361948, -0.01893504,  0.00024943]),\n",
       "  array([-0.02351591,  0.00108344,  0.01301907, -0.00146986, -0.03370089,\n",
       "          0.02917919,  0.02215244, -0.00269818, -0.01068807, -0.01748665,\n",
       "         -0.00024932, -0.00090134,  0.00666417, -0.00540354,  0.00534665,\n",
       "          0.01400693, -0.00442097,  0.0080783 ]),\n",
       "  array([-0.01726204, -0.00730088,  0.03067101, -0.0321546 , -0.00369207,\n",
       "         -0.00240034, -0.02041713,  0.00798676,  0.03914496,  0.02928135,\n",
       "          0.00298133, -0.00496922, -0.00728747, -0.00343598,  0.01795668,\n",
       "         -0.00532421, -0.00087185, -0.00793809, -0.00393357, -0.02118535,\n",
       "         -0.00500613,  0.0070464 ,  0.00690569, -0.00915552]),\n",
       "  array([-0.01108834,  0.00796019,  0.02183295, -0.00056793, -0.00041752,\n",
       "         -0.00297566, -0.00638126,  0.00488044, -0.00836563,  0.02253265,\n",
       "          0.01397209,  0.01145584,  0.00063767,  0.00661705,  0.00477859,\n",
       "         -0.00297739, -0.00743506,  0.01387951,  0.00011055, -0.00834455,\n",
       "         -0.00597571, -0.00819478,  0.00544559, -0.00489284,  0.00141563])],\n",
       " [0.4651956435121739,\n",
       "  0.474697531957048,\n",
       "  0.45932730119435167,\n",
       "  0.48683941341480225])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def StochasticGradientDescent(y, tXst):\n",
    "  # Define the parameters of the algorithm.\n",
    "    max_iters = 100\n",
    "    gamma = 0.001\n",
    "    batch_size = 1\n",
    "  # Initialization\n",
    "    w_list = []\n",
    "    loss_list = []\n",
    "  # Start SGD.\n",
    "    for i in range(4):\n",
    "        w_initial = np.array([0 for i in range(tXst[i].shape[1])])\n",
    "        sgd_losses, sgd_w = stochastic_gradient_descent(y[i], tXst[i], w_initial, batch_size, max_iters, gamma)\n",
    "        w_list.append(sgd_w)\n",
    "        loss_list.append(sgd_losses)\n",
    "    return w_list, loss_list\n",
    "StochasticGradientDescent(y, tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for 0 subset are: 1 0.01  with error:  0.7929677144488401\n",
      "Best parameters for 1 subset are: 2 0.01  with error:  0.8450561850194014\n",
      "Best parameters for 2 subset are: 2 0.01  with error:  0.8173424201564052\n",
      "Best parameters for 3 subset are: 2 0.01  with error:  0.8320016918872625\n",
      "[[0.93630976 0.93645421 0.93732362 0.94137645 0.95317962 0.97125258\n",
      "  0.98617216 0.99424679]\n",
      " [0.79296771 0.7959668  0.80461279 0.82188364 0.85171029 0.89660549\n",
      "  0.94290872 0.97407404]\n",
      " [2.63784812 2.50596282 2.11248506 1.4968839  0.92503176 0.94594117\n",
      "  1.17291633 1.16651706]] [[0.97364297 0.97370116 0.9740531  0.97569909 0.98052021 0.98797365\n",
      "  0.99418946 0.99757674]\n",
      " [0.88634788 0.88770695 0.89194932 0.90119727 0.91843812 0.94433812\n",
      "  0.96988659 0.98647535]\n",
      " [0.84505619 0.84631685 0.84932718 0.85628768 0.8726649  0.90067554\n",
      "  0.93105463 0.95601466]] [[0.99785752 0.99786204 0.99788998 0.9980218  0.99841024 0.99901492\n",
      "  0.99952264 0.9998006 ]\n",
      " [0.85156878 0.85317203 0.85792353 0.86728612 0.88394989 0.91290307\n",
      "  0.94846495 0.97555061]\n",
      " [0.81734242 0.81842084 0.82230268 0.83287073 0.85398572 0.88540182\n",
      "  0.92024966 0.95092966]] [[0.93321668 0.93336927 0.93428432 0.93854342 0.95093514 0.96988973\n",
      "  0.98552227 0.99397763]\n",
      " [0.86587244 0.86764829 0.87254226 0.88319215 0.90506429 0.93751498\n",
      "  0.96714593 0.98541129]\n",
      " [0.83200169 0.83433777 0.83868516 0.8473625  0.86204602 0.88072703\n",
      "  0.90335707 0.93007632]]\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-2, 1, 8)\n",
    "    # split data in k fold\n",
    "    num_degrees=3\n",
    "    # define lists to store the loss of training data and test data\n",
    "    matrix_te0=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr0=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te1=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr1=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te2=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr2=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te3=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr3=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    for degree in np.arange(num_degrees):\n",
    "        for ind_lmbd,lambda_ in enumerate(lambdas):\n",
    "            #losses_te=[]\n",
    "            #losses_tr=[]\n",
    "            for i in range(0,4):\n",
    "                k_indices,indices = build_k_indices(y[i], k_fold, seed)\n",
    "                loss_tr=0\n",
    "                loss_te=0\n",
    "                for k in range(k_fold):\n",
    "                    #print([degree,ind_lmbd,i,k])\n",
    "                    l_tr, l_te,_ = cross_validation_ridge(y[i], tXst[i], k_indices, k, lambda_, degree)\n",
    "                    loss_tr+=l_tr\n",
    "                    loss_te+=l_te\n",
    "                if i==0:\n",
    "                    matrix_te0[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr0[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==1:\n",
    "                    matrix_te1[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr1[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==2:\n",
    "                    matrix_te2[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr2[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==3:\n",
    "                    matrix_te3[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr3[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                #losses_te.append(np.mean(loss_te))\n",
    "                #losses_tr.append(np.mean(loss_tr))\n",
    "            #matrix_tr[degree][ind_lmbd]= losses_tr\n",
    "            #matrix_te[degree][ind_lmbd]=losses_te\n",
    "    \n",
    "   \n",
    "    \n",
    "    #get the best degree lambda couple  \n",
    "    result = np.where(matrix_te0 == np.amin(matrix_te0))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 0 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te0[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te1 == np.amin(matrix_te1))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 1 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te1[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te2 == np.amin(matrix_te2))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 2 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te2[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te3 == np.amin(matrix_te3))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 3 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te3[best_degree][best_lambda])\n",
    "    \n",
    "    print(matrix_te0,matrix_te1,matrix_te2,matrix_te3)\n",
    "    #show cross validation for each degree\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    t=np.copy(x)\n",
    "    t[t>0]=1/(1+np.exp(-t[t>0]))\n",
    "    t[t<0]=np.exp(t[t<0])/(1+np.exp(t[t<0]))\n",
    "    return t\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss=((y+1)/2.0).T.dot(np.log(pred+1e-20)) + (1 - ((y+1)/2.0)).T.dot(np.log(1 - pred+1e-20))\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred-(y+1)/2.0)\n",
    "    return grad\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    loss=calculate_loss(y,tx,w)\n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    w = w-gamma*grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-6\n",
    "    gamma = 0.00001\n",
    "    all_losses=[]\n",
    "    weights=[]\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        losses = []\n",
    "        \n",
    "        # build tx and ty\n",
    "        tx = np.c_[np.ones((y[i].shape[0], 1)), x[i]]\n",
    "        w = np.random.rand(tx.shape[1], 1)\n",
    "        ty=y[i].reshape(y[i].shape[0],1)\n",
    "\n",
    "        print('Training for subset {0}\\n'.format(i))\n",
    "        # start the logistic regression\n",
    "        for iter in range(max_iter):\n",
    "            \n",
    "            # get loss and update w\n",
    "            loss, w = learning_by_gradient_descent(ty, tx, w, gamma)\n",
    "            \n",
    "            # log info\n",
    "            if iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "            \n",
    "            #store loss\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # converge criterion\n",
    "            if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "                break\n",
    "\n",
    "        print(\"\\nBest loss for subset {i}={l}\\n\".format(i=i,l=calculate_loss(ty, tx, w)))\n",
    "        all_losses.append(losses)\n",
    "        weights.append(w)\n",
    "    return all_losses,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for subset 0\n",
      "\n",
      "Current iteration=0, loss=-94886.20696476189\n",
      "Current iteration=100, loss=-34074.19555855833\n",
      "Current iteration=200, loss=-33528.1465416088\n",
      "Current iteration=300, loss=-33359.41626291181\n",
      "Current iteration=400, loss=-33297.208698572635\n",
      "Current iteration=500, loss=-33272.00601237887\n",
      "Current iteration=600, loss=-33261.18326195566\n",
      "Current iteration=700, loss=-33256.34954841683\n",
      "Current iteration=800, loss=-33254.1290540888\n",
      "Current iteration=900, loss=-33253.08727079505\n",
      "Current iteration=1000, loss=-33252.59046749104\n",
      "Current iteration=1100, loss=-33252.35048365808\n",
      "Current iteration=1200, loss=-33252.23335691195\n",
      "Current iteration=1300, loss=-33252.17571409414\n",
      "Current iteration=1400, loss=-33252.14715351522\n",
      "Current iteration=1500, loss=-33252.13292457066\n",
      "Current iteration=1600, loss=-33252.125803966315\n",
      "Current iteration=1700, loss=-33252.12222764617\n",
      "Current iteration=1800, loss=-33252.120426144036\n",
      "Current iteration=1900, loss=-33252.119516503204\n",
      "Current iteration=2000, loss=-33252.119056305455\n",
      "Current iteration=2100, loss=-33252.11882312203\n",
      "\n",
      "Best loss for subset 0=-33252.11872833177\n",
      "\n",
      "Training for subset 1\n",
      "\n",
      "Current iteration=0, loss=-103805.20365451416\n",
      "Current iteration=100, loss=-39795.166956781446\n",
      "Current iteration=200, loss=-39661.55854501513\n",
      "Current iteration=300, loss=-39634.96039464128\n",
      "Current iteration=400, loss=-39628.909568554285\n",
      "Current iteration=500, loss=-39627.47727735728\n",
      "Current iteration=600, loss=-39627.13315195899\n",
      "Current iteration=700, loss=-39627.04994270287\n",
      "Current iteration=800, loss=-39627.02976366988\n",
      "Current iteration=900, loss=-39627.02486321314\n",
      "Current iteration=1000, loss=-39627.02367233397\n",
      "Current iteration=1100, loss=-39627.02338283758\n",
      "\n",
      "Best loss for subset 1=-39627.02335894377\n",
      "\n",
      "Training for subset 2\n",
      "\n",
      "Current iteration=0, loss=-50349.5735360105\n",
      "Current iteration=100, loss=-25113.801951263235\n",
      "Current iteration=200, loss=-25012.9630549699\n",
      "Current iteration=300, loss=-24988.25811848034\n",
      "Current iteration=400, loss=-24981.402613543385\n",
      "Current iteration=500, loss=-24979.4105351978\n",
      "Current iteration=600, loss=-24978.81849126885\n",
      "Current iteration=700, loss=-24978.64031854051\n",
      "Current iteration=800, loss=-24978.586286452766\n",
      "Current iteration=900, loss=-24978.56981800594\n",
      "Current iteration=1000, loss=-24978.564780772285\n",
      "Current iteration=1100, loss=-24978.563235977406\n",
      "Current iteration=1200, loss=-24978.562761266265\n",
      "\n",
      "Best loss for subset 2=-24978.56263340176\n",
      "\n",
      "Training for subset 3\n",
      "\n",
      "Current iteration=0, loss=-28853.5917620084\n",
      "Current iteration=100, loss=-11657.485578951757\n",
      "Current iteration=200, loss=-11438.06716822627\n",
      "Current iteration=300, loss=-11360.070811431637\n",
      "Current iteration=400, loss=-11323.191406673053\n",
      "Current iteration=500, loss=-11304.637951942852\n",
      "Current iteration=600, loss=-11294.914187141592\n",
      "Current iteration=700, loss=-11289.653491552403\n",
      "Current iteration=800, loss=-11286.731640730733\n",
      "Current iteration=900, loss=-11285.070424694204\n",
      "Current iteration=1000, loss=-11284.104450004552\n",
      "Current iteration=1100, loss=-11283.529617876167\n",
      "Current iteration=1200, loss=-11283.178992545694\n",
      "Current iteration=1300, loss=-11282.959331651004\n",
      "Current iteration=1400, loss=-11282.817726991832\n",
      "Current iteration=1500, loss=-11282.723689500586\n",
      "Current iteration=1600, loss=-11282.659362226746\n",
      "Current iteration=1700, loss=-11282.614098121874\n",
      "Current iteration=1800, loss=-11282.581419570728\n",
      "Current iteration=1900, loss=-11282.55729457446\n",
      "Current iteration=2000, loss=-11282.539148803899\n",
      "Current iteration=2100, loss=-11282.5252929357\n",
      "Current iteration=2200, loss=-11282.51458650473\n",
      "Current iteration=2300, loss=-11282.506237709757\n",
      "Current iteration=2400, loss=-11282.499682148191\n",
      "Current iteration=2500, loss=-11282.494507892514\n",
      "Current iteration=2600, loss=-11282.490408135942\n",
      "Current iteration=2700, loss=-11282.487150497464\n",
      "Current iteration=2800, loss=-11282.484556581217\n",
      "Current iteration=2900, loss=-11282.482487984438\n",
      "Current iteration=3000, loss=-11282.480836459978\n",
      "Current iteration=3100, loss=-11282.479516826437\n",
      "Current iteration=3200, loss=-11282.478461745628\n",
      "Current iteration=3300, loss=-11282.47761780348\n",
      "Current iteration=3400, loss=-11282.476942524125\n",
      "Current iteration=3500, loss=-11282.476402067248\n",
      "Current iteration=3600, loss=-11282.475969435596\n",
      "Current iteration=3700, loss=-11282.475623069548\n",
      "Current iteration=3800, loss=-11282.47534573912\n",
      "Current iteration=3900, loss=-11282.47512366695\n",
      "Current iteration=4000, loss=-11282.47494583197\n",
      "Current iteration=4100, loss=-11282.474803415324\n",
      "Current iteration=4200, loss=-11282.474689358729\n",
      "\n",
      "Best loss for subset 3=-11282.474679284762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss,w=logistic_regression_gradient_descent_demo(y,tXst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for subset 0 is 0.7878845372001626\n",
      "Accuracy for subset 1 is 0.6924494870109457\n",
      "Accuracy for subset 2 is 0.7306175807029751\n",
      "Accuracy for subset 3 is 0.7179871416831827\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "for i in range(4):\n",
    "    print('Accuracy for subset {i} is {acc}'.format(i=i,acc=np.sum(np.where(sigmoid(np.c_[np.ones((y[i].shape[0], 1)), tXst[i]]@w[i])>0.5,1,0)==((y[i].reshape(y[i].shape[0],1)+1)/2.0).astype('int'))/y[i].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Penalized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    \n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * (w.T.dot(w))\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    return loss, gradient\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    loss,gradient = penalized_logistic_regression(y,tx,w,lambda_)\n",
    "    w = w-gradient*gamma\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, x, max_iters = 10000,gamma = 0.0001,lambda_ = 0.01):\n",
    "    # init parameters\n",
    "    threshold = 1e-6\n",
    "    all_losses=[]\n",
    "    weights=[]\n",
    "    for i in range(0,len(y)):\n",
    "        losses = []\n",
    "    \n",
    "       # build tx and ty\\n\",\n",
    "        tx = np.c_[np.ones((y[i].shape[0], 1)), x[i]]\n",
    "        w = np.random.rand(tx.shape[1], 1)\n",
    "        ty=y[i].reshape(y[i].shape[0],1)\n",
    "        print('Training for subset {0}\\n'.format(i))\n",
    "        # start the logistic regression\n",
    "        for iter in range(max_iters):\n",
    "    \n",
    "             # get loss and update w.\n",
    "            loss, w = learning_by_penalized_gradient(ty, tx, w, gamma, lambda_)\\\n",
    "           # log info\n",
    "            if iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=iter, l=np.squeeze(loss)))\n",
    "\n",
    "                losses.append(loss)\n",
    "                if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "                    break\n",
    "\n",
    "        print(\"\\nBest loss for subset {i}={l}\\n\".format(i=i,l=calculate_loss(ty, tx, w)))\n",
    "        all_losses.append(losses)\n",
    "        weights.append(w)\n",
    "    return all_losses,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for subset 0\n",
      "\n",
      "Current iteration=0, loss=-87086.59742854706\n",
      "Current iteration=100, loss=-38262.36958564681\n",
      "Current iteration=200, loss=-38228.53855158689\n",
      "Current iteration=300, loss=-38227.83331259681\n",
      "Current iteration=400, loss=-38227.81061658162\n",
      "Current iteration=500, loss=-38227.80964725577\n",
      "Current iteration=600, loss=-38227.80960101806\n",
      "Current iteration=700, loss=-38227.80959873831\n",
      "Current iteration=800, loss=-38227.80959862493\n",
      "Current iteration=900, loss=-38227.80959861927\n",
      "\n",
      "Best loss for subset 0=-41634.19379877557\n",
      "\n",
      "Training for subset 1\n",
      "\n",
      "Current iteration=0, loss=-86437.1472959503\n",
      "Current iteration=100, loss=-52368.28844696923\n",
      "Current iteration=200, loss=-52348.067606342796\n",
      "Current iteration=300, loss=-52347.9374656172\n",
      "Current iteration=400, loss=-52347.936627008705\n",
      "Current iteration=500, loss=-52347.936621604735\n",
      "Current iteration=600, loss=-52347.936621569934\n",
      "Current iteration=700, loss=-52347.936621569715\n",
      "\n",
      "Best loss for subset 1=-61146.758356030376\n",
      "\n",
      "Training for subset 2\n",
      "\n",
      "Current iteration=0, loss=-50896.67791622339\n",
      "Current iteration=100, loss=-31151.912165835307\n",
      "Current iteration=200, loss=-31140.922747571196\n",
      "Current iteration=300, loss=-31140.87009055032\n",
      "Current iteration=400, loss=-31140.869819377163\n",
      "Current iteration=500, loss=-31140.869817920047\n",
      "Current iteration=600, loss=-31140.86981791202\n",
      "\n",
      "Best loss for subset 2=-28796.692545123646\n",
      "\n",
      "Training for subset 3\n",
      "\n",
      "Current iteration=0, loss=-35375.294526317586\n",
      "Current iteration=100, loss=-11288.86901108046\n",
      "Current iteration=200, loss=-11283.040504969646\n",
      "Current iteration=300, loss=-11282.492876812883\n",
      "Current iteration=400, loss=-11282.434540638045\n",
      "Current iteration=500, loss=-11282.428097815615\n",
      "Current iteration=600, loss=-11282.42733804029\n",
      "Current iteration=700, loss=-11282.427233939978\n",
      "Current iteration=800, loss=-11282.427215381245\n",
      "Current iteration=900, loss=-11282.427210969861\n",
      "Current iteration=1000, loss=-11282.427209703406\n",
      "Current iteration=1100, loss=-11282.42720930753\n",
      "Current iteration=1200, loss=-11282.427209179823\n",
      "Current iteration=1300, loss=-11282.427209138175\n",
      "Current iteration=1400, loss=-11282.427209124546\n",
      "Current iteration=1500, loss=-11282.42720912008\n",
      "\n",
      "Best loss for subset 3=-11282.474234276031\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[array([[-87086.59742855]]),\n",
       "   array([[-38262.36958565]]),\n",
       "   array([[-38228.53855159]]),\n",
       "   array([[-38227.8333126]]),\n",
       "   array([[-38227.81061658]]),\n",
       "   array([[-38227.80964726]]),\n",
       "   array([[-38227.80960102]]),\n",
       "   array([[-38227.80959874]]),\n",
       "   array([[-38227.80959862]]),\n",
       "   array([[-38227.80959862]])],\n",
       "  [array([[-86437.14729595]]),\n",
       "   array([[-52368.28844697]]),\n",
       "   array([[-52348.06760634]]),\n",
       "   array([[-52347.93746562]]),\n",
       "   array([[-52347.93662701]]),\n",
       "   array([[-52347.9366216]]),\n",
       "   array([[-52347.93662157]]),\n",
       "   array([[-52347.93662157]])],\n",
       "  [array([[-50896.67791622]]),\n",
       "   array([[-31151.91216584]]),\n",
       "   array([[-31140.92274757]]),\n",
       "   array([[-31140.87009055]]),\n",
       "   array([[-31140.86981938]]),\n",
       "   array([[-31140.86981792]]),\n",
       "   array([[-31140.86981791]])],\n",
       "  [array([[-35375.29452632]]),\n",
       "   array([[-11288.86901108]]),\n",
       "   array([[-11283.04050497]]),\n",
       "   array([[-11282.49287681]]),\n",
       "   array([[-11282.43454064]]),\n",
       "   array([[-11282.42809782]]),\n",
       "   array([[-11282.42733804]]),\n",
       "   array([[-11282.42723394]]),\n",
       "   array([[-11282.42721538]]),\n",
       "   array([[-11282.42721097]]),\n",
       "   array([[-11282.4272097]]),\n",
       "   array([[-11282.42720931]]),\n",
       "   array([[-11282.42720918]]),\n",
       "   array([[-11282.42720914]]),\n",
       "   array([[-11282.42720912]]),\n",
       "   array([[-11282.42720912]])]],\n",
       " [array([[-1.07019922e+00],\n",
       "         [-9.07503614e-01],\n",
       "         [-3.17803904e+00],\n",
       "         [ 1.89520899e+00],\n",
       "         [ 1.77630895e-01],\n",
       "         [-1.29357669e+00],\n",
       "         [ 7.78235614e-02],\n",
       "         [ 1.32257128e+00],\n",
       "         [ 2.43689439e-02],\n",
       "         [ 2.96475311e-03],\n",
       "         [ 6.99181302e-01],\n",
       "         [ 3.54469172e-02],\n",
       "         [ 1.00549522e-02],\n",
       "         [-5.81098370e-01],\n",
       "         [-1.33412040e-02],\n",
       "         [ 2.90686505e-02]]),\n",
       "  array([[-0.94040165],\n",
       "         [-0.52918137],\n",
       "         [-1.57106743],\n",
       "         [ 1.05055579],\n",
       "         [ 0.38198139],\n",
       "         [-0.90412895],\n",
       "         [ 0.38940729],\n",
       "         [ 1.03636166],\n",
       "         [ 0.01027261],\n",
       "         [-0.01422781],\n",
       "         [ 1.81264847],\n",
       "         [-0.03353669],\n",
       "         [-0.01602296],\n",
       "         [ 0.65057948],\n",
       "         [ 0.00406002],\n",
       "         [ 0.54779193],\n",
       "         [ 1.3215038 ],\n",
       "         [ 0.00644213],\n",
       "         [-0.00917278]]),\n",
       "  array([[ 0.4131945 ],\n",
       "         [-0.49797498],\n",
       "         [-0.7041681 ],\n",
       "         [ 1.13917962],\n",
       "         [ 0.27632127],\n",
       "         [ 1.1359157 ],\n",
       "         [-0.21959888],\n",
       "         [-0.61372742],\n",
       "         [ 0.31891696],\n",
       "         [ 0.45941114],\n",
       "         [ 0.80596733],\n",
       "         [-0.01081574],\n",
       "         [-0.00274879],\n",
       "         [ 1.05167331],\n",
       "         [-0.02587147],\n",
       "         [ 0.02184307],\n",
       "         [ 0.46689934],\n",
       "         [ 0.01782554],\n",
       "         [-0.64208978],\n",
       "         [ 0.0204899 ],\n",
       "         [ 0.02400244],\n",
       "         [ 0.12098275],\n",
       "         [ 0.01994511],\n",
       "         [-0.01340423],\n",
       "         [-0.99417429]]),\n",
       "  array([[-0.92142314],\n",
       "         [-0.32010088],\n",
       "         [-0.43679182],\n",
       "         [ 0.94159797],\n",
       "         [-0.33888609],\n",
       "         [ 0.61699255],\n",
       "         [ 0.08973679],\n",
       "         [ 0.75946069],\n",
       "         [ 0.02590808],\n",
       "         [-0.60414906],\n",
       "         [ 0.23088777],\n",
       "         [ 0.27896773],\n",
       "         [ 0.10842716],\n",
       "         [-0.02681518],\n",
       "         [-0.00104308],\n",
       "         [ 0.47296689],\n",
       "         [-0.00919603],\n",
       "         [ 0.00947717],\n",
       "         [-0.07125768],\n",
       "         [ 0.01201446],\n",
       "         [-0.00487555],\n",
       "         [-0.00878152],\n",
       "         [ 0.07950783],\n",
       "         [ 0.01875185],\n",
       "         [ 0.00512218],\n",
       "         [-0.92092855]])])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_penalized_gradient_descent_demo(y, tXst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic_regression(y, x, k_indices,max_iter, k, gamma,lambda_):\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    x_train = x[tr_indice]\n",
    "    y_train = y[tr_indice]\n",
    "    threshold = 1e-6\n",
    "    \n",
    "    w = np.random.rand(x.shape[1], 1)\n",
    "    # get loss and update w.\n",
    "    losses=[]\n",
    "    for iter in range(max_iter):\n",
    "        l_tr,w = learning_by_penalized_gradient(y_train, x_train, w, gamma, lambda_)\n",
    "        # log info\n",
    "        losses.append(l_tr)\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=np.squeeze(l_tr)))\n",
    "        # converge criterion\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    loss_tr = losses[-1]\n",
    "    loss_te= calculate_loss(y_test, x_test, w)\n",
    "\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 2000\n",
    "    gammas = [0.0001]\n",
    "    lambdas = [0.01]\n",
    "\n",
    "    all_losses=[]\n",
    "    weights=[]\n",
    "    k_fold = 4\n",
    "    seed = 1\n",
    "    matrix_te0=np.zeros(shape=(len(gammas),len(lambdas)))\n",
    "    matrix_te1=np.zeros(shape=(len(gammas),len(lambdas)))\n",
    "    matrix_te2=np.zeros(shape=(len(gammas),len(lambdas)))\n",
    "    matrix_te3=np.zeros(shape=(len(gammas),len(lambdas)))\n",
    "\n",
    "    for i in range(0,len(y)):\n",
    "        k_indices,indices = build_k_indices(y[i], k_fold, seed)\n",
    "\n",
    "        # build tx and ty\n",
    "        tx = np.c_[np.ones((y[i].shape[0], 1)), x[i]]\n",
    "        ty=y[i].reshape(y[i].shape[0],1)\n",
    "       \n",
    "        print('Training for subset {0}\\n'.format(i))\n",
    "        # start the logistic regression\n",
    "        for ind_gm,gamma in enumerate(gammas):\n",
    "            for ind_lmbd,lambda_ in enumerate(lambdas):\n",
    "                loss_te=0\n",
    "                for k in range(k_fold):\n",
    "                    l_tr,l_te=cross_validation_logistic_regression(ty, tx, k_indices,max_iter, k, gamma,lambda_)\n",
    "                    loss_te+=l_te\n",
    "                if i==0:\n",
    "                    matrix_te0[ind_gm][ind_lmbd]=loss_te\n",
    "                elif i==1:\n",
    "                    matrix_te1[ind_gm][ind_lmbd]=loss_te\n",
    "                elif i==2:\n",
    "                    matrix_te2[ind_gm][ind_lmbd]=loss_te\n",
    "                elif i==3:\n",
    "                    matrix_te3[ind_gm][ind_lmbd]=loss_te\n",
    "\n",
    "    result = np.where(matrix_te0 == np.amin(matrix_te0))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_gamma = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 0 subset are:',gammas[best_gamma],lambdas[best_lambda],' with error: ',matrix_te0[best_gamma][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te1 == np.amin(matrix_te1))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_gamma = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 1 subset are:',gammas[best_gamma],lambdas[best_lambda],' with error: ',matrix_te1[best_gamma][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te2 == np.amin(matrix_te2))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_gamma = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 2 subset are:',gammas[best_gamma],lambdas[best_lambda],' with error: ',matrix_te2[best_gamma][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te3 == np.amin(matrix_te3))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_gamma = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 3 subset are:',gammas[best_gamma],lambdas[best_lambda],' with error: ',matrix_te3[best_gamma][best_lambda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for subset 0\n",
      "\n",
      "Current iteration=0, loss=-79693.56666204367\n",
      "Current iteration=100, loss=-24902.681151507677\n",
      "Current iteration=200, loss=-24898.961706125283\n",
      "Current iteration=300, loss=-24898.939356634404\n",
      "Current iteration=0, loss=-85534.50317046499\n",
      "Current iteration=100, loss=-24965.26431990047\n",
      "Current iteration=200, loss=-24963.28460202133\n",
      "Current iteration=300, loss=-24963.274212070268\n",
      "Current iteration=0, loss=-81298.66836860028\n",
      "Current iteration=100, loss=-24993.06800695182\n",
      "Current iteration=200, loss=-24990.580085344995\n",
      "Current iteration=300, loss=-24990.566619231726\n",
      "Current iteration=0, loss=-96870.32202113386\n",
      "Current iteration=100, loss=-24884.111895406\n",
      "Current iteration=200, loss=-24881.07193429545\n",
      "Current iteration=300, loss=-24881.05480022465\n",
      "Training for subset 1\n",
      "\n",
      "Current iteration=0, loss=-69852.29851089217\n",
      "Current iteration=100, loss=-32131.119060250454\n",
      "Current iteration=200, loss=-32125.966774629076\n",
      "Current iteration=300, loss=-32125.94469168458\n",
      "Current iteration=400, loss=-32125.944596697216\n",
      "Current iteration=500, loss=-32125.944596288577\n",
      "Current iteration=600, loss=-32125.94459628688\n",
      "Current iteration=700, loss=-32125.944596286874\n",
      "Current iteration=800, loss=-32125.944596286878\n",
      "Current iteration=900, loss=-32125.944596286874\n",
      "Current iteration=1000, loss=-32125.944596286874\n",
      "Current iteration=1100, loss=-32125.944596286874\n",
      "Current iteration=1200, loss=-32125.94459628688\n",
      "Current iteration=1300, loss=-32125.94459628688\n",
      "Current iteration=1400, loss=-32125.944596286874\n",
      "Current iteration=1500, loss=-32125.944596286874\n",
      "Current iteration=1600, loss=-32125.944596286874\n",
      "Current iteration=1700, loss=-32125.944596286874\n",
      "Current iteration=1800, loss=-32125.944596286874\n",
      "Current iteration=1900, loss=-32125.944596286878\n",
      "Current iteration=0, loss=-81154.75375331487\n",
      "Current iteration=100, loss=-32212.60347743472\n",
      "Current iteration=200, loss=-32205.629098842215\n",
      "Current iteration=300, loss=-32205.59647056429\n",
      "Current iteration=400, loss=-32205.596317260133\n",
      "Current iteration=500, loss=-32205.59631653981\n",
      "Current iteration=600, loss=-32205.59631653637\n",
      "Current iteration=700, loss=-32205.596316536346\n",
      "Current iteration=800, loss=-32205.596316536346\n",
      "Current iteration=900, loss=-32205.596316536354\n",
      "Current iteration=1000, loss=-32205.596316536354\n",
      "Current iteration=1100, loss=-32205.596316536354\n",
      "Current iteration=1200, loss=-32205.596316536354\n",
      "Current iteration=1300, loss=-32205.596316536354\n",
      "Current iteration=1400, loss=-32205.596316536346\n",
      "Current iteration=1500, loss=-32205.596316536354\n",
      "Current iteration=1600, loss=-32205.596316536354\n",
      "Current iteration=1700, loss=-32205.596316536354\n",
      "Current iteration=1800, loss=-32205.596316536354\n",
      "Current iteration=1900, loss=-32205.596316536346\n",
      "Current iteration=0, loss=-69022.1444220526\n",
      "Current iteration=100, loss=-32337.47912683915\n",
      "Current iteration=200, loss=-32330.298588180183\n",
      "Current iteration=300, loss=-32330.263177463843\n",
      "Current iteration=400, loss=-32330.26300200019\n",
      "Current iteration=500, loss=-32330.263001130756\n",
      "Current iteration=600, loss=-32330.263001126496\n",
      "Current iteration=700, loss=-32330.26300112647\n",
      "Current iteration=800, loss=-32330.26300112647\n",
      "Current iteration=900, loss=-32330.263001126466\n",
      "Current iteration=1000, loss=-32330.26300112647\n",
      "Current iteration=1100, loss=-32330.263001126466\n",
      "Current iteration=1200, loss=-32330.263001126466\n",
      "Current iteration=1300, loss=-32330.26300112647\n",
      "Current iteration=1400, loss=-32330.263001126463\n",
      "Current iteration=1500, loss=-32330.263001126466\n",
      "Current iteration=1600, loss=-32330.26300112647\n",
      "Current iteration=1700, loss=-32330.26300112647\n",
      "Current iteration=1800, loss=-32330.26300112647\n",
      "Current iteration=1900, loss=-32330.26300112647\n",
      "Current iteration=0, loss=-65815.9311264679\n",
      "Current iteration=100, loss=-34270.926984195125\n",
      "Current iteration=200, loss=-34257.180318652325\n",
      "Current iteration=300, loss=-34257.11531680187\n",
      "Current iteration=400, loss=-34257.11501078927\n",
      "Current iteration=500, loss=-34257.11500934408\n",
      "Current iteration=600, loss=-34257.11500933754\n",
      "Current iteration=700, loss=-34257.11500933752\n",
      "Current iteration=800, loss=-34257.11500933751\n",
      "Current iteration=900, loss=-34257.11500933752\n",
      "Current iteration=1000, loss=-34257.11500933751\n",
      "Current iteration=1100, loss=-34257.11500933751\n",
      "Current iteration=1200, loss=-34257.11500933752\n",
      "Current iteration=1300, loss=-34257.115009337525\n",
      "Current iteration=1400, loss=-34257.11500933752\n",
      "Current iteration=1500, loss=-34257.11500933751\n",
      "Current iteration=1600, loss=-34257.11500933751\n",
      "Current iteration=1700, loss=-34257.11500933751\n",
      "Current iteration=1800, loss=-34257.11500933751\n",
      "Current iteration=1900, loss=-34257.11500933751\n",
      "Training for subset 2\n",
      "\n",
      "Current iteration=0, loss=-36194.27560580308\n",
      "Current iteration=100, loss=-18721.416545268596\n",
      "Current iteration=200, loss=-18720.815801178644\n",
      "Current iteration=0, loss=-51615.365064584665\n",
      "Current iteration=100, loss=-18785.89678856671\n",
      "Current iteration=200, loss=-18779.083925503994\n",
      "Current iteration=300, loss=-18778.930165191516\n",
      "Current iteration=400, loss=-18778.926462308507\n",
      "Current iteration=0, loss=-40080.77232853599\n",
      "Current iteration=100, loss=-18626.91130435073\n",
      "Current iteration=200, loss=-18624.542646276106\n",
      "Current iteration=0, loss=-39101.57104774573\n",
      "Current iteration=100, loss=-18799.76747928503\n",
      "Current iteration=200, loss=-18792.592616515085\n",
      "Current iteration=300, loss=-18792.540392085357\n",
      "Training for subset 3\n",
      "\n",
      "Current iteration=0, loss=-21079.441434089153\n",
      "Current iteration=100, loss=-8459.029352440402\n",
      "Current iteration=200, loss=-8442.507824207683\n",
      "Current iteration=300, loss=-8439.732155474341\n",
      "Current iteration=400, loss=-8439.198345234485\n",
      "Current iteration=500, loss=-8439.09270057639\n",
      "Current iteration=600, loss=-8439.071409022414\n",
      "Current iteration=700, loss=-8439.067017216272\n",
      "Current iteration=800, loss=-8439.066073417092\n",
      "Current iteration=900, loss=-8439.06585483355\n",
      "Current iteration=0, loss=-25335.726181431644\n",
      "Current iteration=100, loss=-8447.83734109054\n",
      "Current iteration=200, loss=-8438.319030099155\n",
      "Current iteration=300, loss=-8437.442461278983\n",
      "Current iteration=400, loss=-8437.296181038233\n",
      "Current iteration=500, loss=-8437.269335703297\n",
      "Current iteration=600, loss=-8437.264251577444\n",
      "Current iteration=700, loss=-8437.263246085236\n",
      "Current iteration=0, loss=-25261.54087014114\n",
      "Current iteration=100, loss=-8504.226966059374\n",
      "Current iteration=200, loss=-8492.148153862407\n",
      "Current iteration=300, loss=-8490.290155042409\n",
      "Current iteration=400, loss=-8489.964652829569\n",
      "Current iteration=500, loss=-8489.906281083706\n",
      "Current iteration=600, loss=-8489.895621844504\n",
      "Current iteration=700, loss=-8489.893618187523\n",
      "Current iteration=800, loss=-8489.893219660698\n",
      "Current iteration=0, loss=-24011.13115381041\n",
      "Current iteration=100, loss=-8470.921224161384\n",
      "Current iteration=200, loss=-8460.657494523053\n",
      "Current iteration=300, loss=-8459.051752447413\n",
      "Current iteration=400, loss=-8458.755632536066\n",
      "Current iteration=500, loss=-8458.699299657419\n",
      "Current iteration=600, loss=-8458.688347372472\n",
      "Current iteration=700, loss=-8458.686150687845\n",
      "Current iteration=800, loss=-8458.685684270007\n",
      "Best parameters for 0 subset are: 0.0001 0.01  with error:  -33290.83217653277\n",
      "Best parameters for 1 subset are: 0.0001 0.01  with error:  -43568.570039826714\n",
      "Best parameters for 2 subset are: 0.0001 0.01  with error:  -25005.08102947173\n",
      "Best parameters for 3 subset are: 0.0001 0.01  with error:  -11312.779309212487\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_penalized_gradient_descent_demo(y, tXst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.00001\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    all_losses=[]\n",
    "    weights=[]\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        losses = []\n",
    "        \n",
    "        # build tx and ty\n",
    "        tx = np.c_[np.ones((y[i].shape[0], 1)), x[i]]\n",
    "        w = np.random.rand(tx.shape[1], 1)\n",
    "        ty=y[i].reshape(y[i].shape[0],1)\n",
    "        \n",
    "        print('Training for subset {0}\\n'.format(i))\n",
    "        # start the logistic regression\n",
    "        for iter in range(max_iter):\n",
    "            \n",
    "            # get loss and update w.\n",
    "            loss, w = learning_by_penalized_gradient(ty, tx, w, gamma, lambda_)\n",
    "            \n",
    "            # log info\n",
    "            if iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=iter, l=np.squeeze(loss)))\n",
    "            # converge criterion\n",
    "            losses.append(loss)\n",
    "            if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "                break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\",True)\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "        print(\"\\nBest loss for subset {i}={l}\\n\".format(i=i,l=calculate_loss(ty, tx, w)))\n",
    "        all_losses.append(losses)\n",
    "        weights.append(w)\n",
    "    return all_losses,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for subset 0\n",
      "\n",
      "Current iteration=0, loss=[[-118692.63740843]]\n",
      "Current iteration=100, loss=[[-33929.39805208]]\n",
      "Current iteration=200, loss=[[-33492.08202483]]\n",
      "Current iteration=300, loss=[[-33348.29032852]]\n",
      "Current iteration=400, loss=[[-33293.08509529]]\n",
      "Current iteration=500, loss=[[-33269.97014293]]\n",
      "Current iteration=600, loss=[[-33259.7502091]]\n",
      "Current iteration=700, loss=[[-33255.05998517]]\n",
      "Current iteration=800, loss=[[-33252.84781388]]\n",
      "Current iteration=900, loss=[[-33251.78190969]]\n",
      "Current iteration=1000, loss=[[-33251.25912803]]\n",
      "Current iteration=1100, loss=[[-33250.99866268]]\n",
      "Current iteration=1200, loss=[[-33250.86694308]]\n",
      "Current iteration=1300, loss=[[-33250.79932134]]\n",
      "Current iteration=1400, loss=[[-33250.76404525]]\n",
      "Current iteration=1500, loss=[[-33250.74531453]]\n",
      "Current iteration=1600, loss=[[-33250.73516964]]\n",
      "Current iteration=1700, loss=[[-33250.7295514]]\n",
      "Current iteration=1800, loss=[[-33250.72636285]]\n",
      "Current iteration=1900, loss=[[-33250.72450517]]\n",
      "Current iteration=2000, loss=[[-33250.72339326]]\n",
      "Current iteration=2100, loss=[[-33250.72270977]]\n",
      "Current iteration=2200, loss=[[-33250.72227892]]\n",
      "Current iteration=2300, loss=[[-33250.7220011]]\n",
      "Current iteration=2400, loss=[[-33250.72181838]]\n",
      "Current iteration=2500, loss=[[-33250.72169619]]\n",
      "Current iteration=2600, loss=[[-33250.72161336]]\n",
      "Current iteration=2700, loss=[[-33250.7215566]]\n",
      "Current iteration=2800, loss=[[-33250.72151738]]\n",
      "Current iteration=2900, loss=[[-33250.72149009]]\n",
      "Current iteration=3000, loss=[[-33250.721471]]\n",
      "Current iteration=3100, loss=[[-33250.7214576]]\n",
      "Current iteration=3200, loss=[[-33250.72144816]]\n",
      "Current iteration=3300, loss=[[-33250.7214415]]\n",
      "Current iteration=3400, loss=[[-33250.72143678]]\n",
      "Current iteration=3500, loss=[[-33250.72143344]]\n",
      "Current iteration=3600, loss=[[-33250.72143107]]\n",
      "Current iteration=3700, loss=[[-33250.72142939]]\n",
      "Current iteration=3800, loss=[[-33250.7214282]]\n",
      "\n",
      "Best loss for subset 0=-33252.119200145185\n",
      "\n",
      "Training for subset 1\n",
      "\n",
      "Current iteration=0, loss=[[-105511.48961518]]\n",
      "Current iteration=100, loss=[[-39855.97171067]]\n",
      "Current iteration=200, loss=[[-39670.19679544]]\n",
      "Current iteration=300, loss=[[-39636.44226907]]\n",
      "Current iteration=400, loss=[[-39628.97025658]]\n",
      "Current iteration=500, loss=[[-39627.20822237]]\n",
      "Current iteration=600, loss=[[-39626.78087583]]\n",
      "Current iteration=700, loss=[[-39626.67496705]]\n",
      "Current iteration=800, loss=[[-39626.64795535]]\n",
      "Current iteration=900, loss=[[-39626.64073439]]\n",
      "Current iteration=1000, loss=[[-39626.63865269]]\n",
      "Current iteration=1100, loss=[[-39626.63798531]]\n",
      "Current iteration=1200, loss=[[-39626.63774347]]\n",
      "Current iteration=1300, loss=[[-39626.63764544]]\n",
      "Current iteration=1400, loss=[[-39626.63760226]]\n",
      "Current iteration=1500, loss=[[-39626.63758222]]\n",
      "Current iteration=1600, loss=[[-39626.63757265]]\n",
      "Current iteration=1700, loss=[[-39626.637568]]\n",
      "Current iteration=1800, loss=[[-39626.63756573]]\n",
      "\n",
      "Best loss for subset 1=-39627.02336479403\n",
      "\n",
      "Training for subset 2\n",
      "\n",
      "Current iteration=0, loss=[[-70084.10399107]]\n",
      "Current iteration=100, loss=[[-25249.24231215]]\n",
      "Current iteration=200, loss=[[-25046.95232714]]\n",
      "Current iteration=300, loss=[[-24997.71970144]]\n",
      "Current iteration=400, loss=[[-24983.94147017]]\n",
      "Current iteration=500, loss=[[-24979.89796327]]\n",
      "Current iteration=600, loss=[[-24978.67718949]]\n",
      "Current iteration=700, loss=[[-24978.301062]]\n",
      "Current iteration=800, loss=[[-24978.18304024]]\n",
      "Current iteration=900, loss=[[-24978.14520034]]\n",
      "Current iteration=1000, loss=[[-24978.13269435]]\n",
      "Current iteration=1100, loss=[[-24978.12837289]]\n",
      "Current iteration=1200, loss=[[-24978.12678381]]\n",
      "Current iteration=1300, loss=[[-24978.12615218]]\n",
      "Current iteration=1400, loss=[[-24978.12587903]]\n",
      "Current iteration=1500, loss=[[-24978.12575135]]\n",
      "Current iteration=1600, loss=[[-24978.12568786]]\n",
      "Current iteration=1700, loss=[[-24978.12565488]]\n",
      "Current iteration=1800, loss=[[-24978.12563726]]\n",
      "Current iteration=1900, loss=[[-24978.12562768]]\n",
      "Current iteration=2000, loss=[[-24978.12562243]]\n",
      "Current iteration=2100, loss=[[-24978.12561952]]\n",
      "Current iteration=2200, loss=[[-24978.12561792]]\n",
      "\n",
      "Best loss for subset 2=-24978.562659013565\n",
      "\n",
      "Training for subset 3\n",
      "\n",
      "Current iteration=0, loss=[[-30515.67795241]]\n",
      "Current iteration=100, loss=[[-11599.30206897]]\n",
      "Current iteration=200, loss=[[-11429.55478516]]\n",
      "Current iteration=300, loss=[[-11373.76135486]]\n",
      "Current iteration=400, loss=[[-11344.63426971]]\n",
      "Current iteration=500, loss=[[-11326.88434996]]\n",
      "Current iteration=600, loss=[[-11315.19696423]]\n",
      "Current iteration=700, loss=[[-11307.10047653]]\n",
      "Current iteration=800, loss=[[-11301.27825409]]\n",
      "Current iteration=900, loss=[[-11296.97280468]]\n",
      "Current iteration=1000, loss=[[-11293.72234659]]\n",
      "Current iteration=1100, loss=[[-11291.23087008]]\n",
      "Current iteration=1200, loss=[[-11289.29999341]]\n",
      "Current iteration=1300, loss=[[-11287.79156357]]\n",
      "Current iteration=1400, loss=[[-11286.60626311]]\n",
      "Current iteration=1500, loss=[[-11285.67085952]]\n",
      "Current iteration=1600, loss=[[-11284.93028475]]\n",
      "Current iteration=1700, loss=[[-11284.34251079]]\n",
      "Current iteration=1800, loss=[[-11283.87510491]]\n",
      "Current iteration=1900, loss=[[-11283.50283272]]\n",
      "Current iteration=2000, loss=[[-11283.20594077]]\n",
      "Current iteration=2100, loss=[[-11282.96889562]]\n",
      "Current iteration=2200, loss=[[-11282.77944042]]\n",
      "Current iteration=2300, loss=[[-11282.62787829]]\n",
      "Current iteration=2400, loss=[[-11282.50652188]]\n",
      "Current iteration=2500, loss=[[-11282.40926691]]\n",
      "Current iteration=2600, loss=[[-11282.33125961]]\n",
      "Current iteration=2700, loss=[[-11282.26863624]]\n",
      "Current iteration=2800, loss=[[-11282.21831811]]\n",
      "Current iteration=2900, loss=[[-11282.17784995]]\n",
      "Current iteration=3000, loss=[[-11282.14527218]]\n",
      "Current iteration=3100, loss=[[-11282.11901968]]\n",
      "Current iteration=3200, loss=[[-11282.09784155]]\n",
      "Current iteration=3300, loss=[[-11282.08073739]]\n",
      "Current iteration=3400, loss=[[-11282.06690658]]\n",
      "Current iteration=3500, loss=[[-11282.05570805]]\n",
      "Current iteration=3600, loss=[[-11282.0466281]]\n",
      "Current iteration=3700, loss=[[-11282.03925487]]\n",
      "Current iteration=3800, loss=[[-11282.03325791]]\n",
      "Current iteration=3900, loss=[[-11282.02837193]]\n",
      "Current iteration=4000, loss=[[-11282.02438378]]\n",
      "Current iteration=4100, loss=[[-11282.02112208]]\n",
      "Current iteration=4200, loss=[[-11282.01844893]]\n",
      "Current iteration=4300, loss=[[-11282.01625329]]\n",
      "Current iteration=4400, loss=[[-11282.01444564]]\n",
      "Current iteration=4500, loss=[[-11282.01295374]]\n",
      "Current iteration=4600, loss=[[-11282.01171927]]\n",
      "Current iteration=4700, loss=[[-11282.01069506]]\n",
      "Current iteration=4800, loss=[[-11282.00984291]]\n",
      "Current iteration=4900, loss=[[-11282.00913188]]\n",
      "Current iteration=5000, loss=[[-11282.00853684]]\n",
      "Current iteration=5100, loss=[[-11282.00803736]]\n",
      "Current iteration=5200, loss=[[-11282.0076168]]\n",
      "Current iteration=5300, loss=[[-11282.00726159]]\n",
      "Current iteration=5400, loss=[[-11282.00696066]]\n",
      "Current iteration=5500, loss=[[-11282.0067049]]\n",
      "Current iteration=5600, loss=[[-11282.00648688]]\n",
      "Current iteration=5700, loss=[[-11282.00630046]]\n",
      "Current iteration=5800, loss=[[-11282.00614058]]\n",
      "Current iteration=5900, loss=[[-11282.00600308]]\n",
      "Current iteration=6000, loss=[[-11282.00588449]]\n",
      "Current iteration=6100, loss=[[-11282.00578193]]\n",
      "Current iteration=6200, loss=[[-11282.00569301]]\n",
      "Current iteration=6300, loss=[[-11282.00561573]]\n",
      "Current iteration=6400, loss=[[-11282.0055484]]\n",
      "Current iteration=6500, loss=[[-11282.00548962]]\n",
      "Current iteration=6600, loss=[[-11282.00543819]]\n",
      "Current iteration=6700, loss=[[-11282.00539311]]\n",
      "Current iteration=6800, loss=[[-11282.00535352]]\n",
      "Current iteration=6900, loss=[[-11282.00531869]]\n",
      "Current iteration=7000, loss=[[-11282.00528802]]\n",
      "Current iteration=7100, loss=[[-11282.00526095]]\n",
      "Current iteration=7200, loss=[[-11282.00523704]]\n",
      "Current iteration=7300, loss=[[-11282.00521589]]\n",
      "Current iteration=7400, loss=[[-11282.00519716]]\n",
      "Current iteration=7500, loss=[[-11282.00518057]]\n",
      "Current iteration=7600, loss=[[-11282.00516584]]\n",
      "Current iteration=7700, loss=[[-11282.00515277]]\n",
      "Current iteration=7800, loss=[[-11282.00514115]]\n",
      "Current iteration=7900, loss=[[-11282.00513082]]\n",
      "Current iteration=8000, loss=[[-11282.00512163]]\n",
      "Current iteration=8100, loss=[[-11282.00511345]]\n",
      "Current iteration=8200, loss=[[-11282.00510616]]\n",
      "Current iteration=8300, loss=[[-11282.00509966]]\n",
      "Current iteration=8400, loss=[[-11282.00509387]]\n",
      "Current iteration=8500, loss=[[-11282.00508871]]\n",
      "Current iteration=8600, loss=[[-11282.0050841]]\n",
      "Current iteration=8700, loss=[[-11282.00507999]]\n",
      "Current iteration=8800, loss=[[-11282.00507632]]\n",
      "Current iteration=8900, loss=[[-11282.00507305]]\n",
      "Current iteration=9000, loss=[[-11282.00507012]]\n",
      "Current iteration=9100, loss=[[-11282.00506751]]\n",
      "Current iteration=9200, loss=[[-11282.00506517]]\n",
      "Current iteration=9300, loss=[[-11282.00506309]]\n",
      "Current iteration=9400, loss=[[-11282.00506122]]\n",
      "Current iteration=9500, loss=[[-11282.00505956]]\n",
      "Current iteration=9600, loss=[[-11282.00505807]]\n",
      "Current iteration=9700, loss=[[-11282.00505673]]\n",
      "Current iteration=9800, loss=[[-11282.00505554]]\n",
      "Current iteration=9900, loss=[[-11282.00505448]]\n",
      "\n",
      "Best loss for subset 3=-11282.474615737123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss,w=logistic_regression_penalized_gradient_descent_demo(y, tXst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for subset 0 is 0.7878980891719746\n",
      "Accuracy for subset 1 is 0.6924351976222457\n",
      "Accuracy for subset 2 is 0.7306386657389251\n",
      "Accuracy for subset 3 is 0.7180838207569972\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "w=loss[:]\n",
    "for i in range(4):\n",
    "    print('Accuracy for subset {i} is {acc}'.format(i=i,acc=np.sum(np.where(sigmoid(np.c_[np.ones((y[i].shape[0], 1)), tXst[i]]@w[i])>0.5,1,0)==((y[i].reshape(y[i].shape[0],1)+1)/2.0).astype('int'))/y[i].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Don't see the correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_12=standartize(tX0cl)\n",
    "tX1_12=standartize(tX1cl)\n",
    "tX2_12=standartize(tX2cl)\n",
    "tX3_12=standartize(tX3cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_12..tX3_12 to y0d...y3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Substitute rows with NaNs in the first column by the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean for 1st column without -999.0 values\n",
    "mean0=tX0[tX0[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean1=tX1[tX1[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean2=tX2[tX2[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean3=tX3[tX3[:,0]!=-999.0].mean(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX0=np.copy(np.where(tX0==-999.0,mean0,tX0))\n",
    "tX1=np.copy(np.where(tX1==-999.0,mean1,tX1))\n",
    "tX2=np.copy(np.where(tX2==-999.0,mean2,tX2))\n",
    "tX3=np.copy(np.where(tX3==-999.0,mean3,tX3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. See the correlation between regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.7488162009965309\n",
      "Pearson correlation between column 3 and 5 : 0.9999999999988465\n",
      "Pearson correlation between column 6 and 9 : 0.8002688533328596\n",
      "Pearson correlation between column 6 and 12 : 0.7797439568172713\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX0\n",
    "for i in range(len(tX0[0,:])):\n",
    "    for j in range(i+1,len(tX0[0,:])):\n",
    "        corr = np.corrcoef(tX0[:,i],tX0[:,j])\n",
    "        if np.abs(np.corrcoef(tX0[:,i],tX0[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it makes sense to drop columns 0,3,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX0=tX0[:,[i for i in range(tX0.shape[1]) if i not in [0,3,6]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8445009835185863\n",
      "Pearson correlation between column 3 and 6 : 0.8632110677076686\n",
      "Pearson correlation between column 3 and 17 : 0.7125818159025532\n",
      "Pearson correlation between column 3 and 18 : 0.9367590332144766\n",
      "Pearson correlation between column 3 and 21 : 0.9367590311733165\n",
      "Pearson correlation between column 6 and 17 : 0.757990128208487\n",
      "Pearson correlation between column 6 and 18 : 0.9051662981927852\n",
      "Pearson correlation between column 6 and 21 : 0.905166300382275\n",
      "Pearson correlation between column 7 and 12 : 0.709867998636906\n",
      "Pearson correlation between column 17 and 18 : 0.7068771933564622\n",
      "Pearson correlation between column 17 and 21 : 0.7068771905369512\n",
      "Pearson correlation between column 18 and 21 : 0.999999999999017\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX1\n",
    "for i in range(len(tX1[0,:])):\n",
    "    for j in range(i+1,len(tX1[0,:])):\n",
    "        corr = np.corrcoef(tX1[:,i],tX1[:,j])\n",
    "        if np.abs(np.corrcoef(tX1[:,i],tX1[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop columns 0,3,6,21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX1=tX1[:,[i for i in range(tX1.shape[1]) if i not in [0,3,6,21]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8228939521810881\n",
      "Pearson correlation between column 3 and 9 : 0.800559238241552\n",
      "Pearson correlation between column 3 and 19 : 0.7285433946032545\n",
      "Pearson correlation between column 3 and 21 : 0.7069249354905159\n",
      "Pearson correlation between column 3 and 22 : 0.7995787774292449\n",
      "Pearson correlation between column 3 and 28 : 0.7584645222360212\n",
      "Pearson correlation between column 4 and 5 : 0.8098715851425442\n",
      "Pearson correlation between column 4 and 6 : -0.8501315483608742\n",
      "Pearson correlation between column 5 and 6 : -0.7780946604694609\n",
      "Pearson correlation between column 9 and 21 : 0.8523147287795626\n",
      "Pearson correlation between column 9 and 22 : 0.9191164357409889\n",
      "Pearson correlation between column 9 and 28 : 0.9397870414625126\n",
      "Pearson correlation between column 10 and 16 : 0.7438565749365871\n",
      "Pearson correlation between column 21 and 22 : 0.7970772581576845\n",
      "Pearson correlation between column 21 and 28 : 0.8147451242277628\n",
      "Pearson correlation between column 22 and 28 : 0.9584459537718197\n",
      "Pearson correlation between column 25 and 28 : 0.720740253151538\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX2\n",
    "for i in range(len(tX2[0,:])):\n",
    "    for j in range(i+1,len(tX2[0,:])):\n",
    "        corr = np.corrcoef(tX2[:,i],tX2[:,j])\n",
    "        if np.abs(np.corrcoef(tX2[:,i],tX2[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,3,4,9,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX2=tX2[:,[i for i in range(tX2.shape[1]) if i not in [0,3,4,9,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8172288262177909\n",
      "Pearson correlation between column 3 and 19 : 0.7502599406025436\n",
      "Pearson correlation between column 4 and 5 : 0.758966330984028\n",
      "Pearson correlation between column 4 and 6 : -0.7735086978837818\n",
      "Pearson correlation between column 9 and 21 : 0.9160946827155181\n",
      "Pearson correlation between column 9 and 22 : 0.8813251894172176\n",
      "Pearson correlation between column 9 and 25 : 0.7708057103889608\n",
      "Pearson correlation between column 9 and 28 : 0.9569498612168339\n",
      "Pearson correlation between column 10 and 16 : 0.7683893227784884\n",
      "Pearson correlation between column 21 and 22 : 0.8132272408011297\n",
      "Pearson correlation between column 21 and 25 : 0.7219329222297065\n",
      "Pearson correlation between column 21 and 28 : 0.892063808128829\n",
      "Pearson correlation between column 22 and 28 : 0.887903038587193\n",
      "Pearson correlation between column 25 and 28 : 0.8365333266992916\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX3\n",
    "for i in range(len(tX3[0,:])):\n",
    "    for j in range(i+1,len(tX3[0,:])):\n",
    "        corr = np.corrcoef(tX3[:,i],tX3[:,j])\n",
    "        if np.abs(np.corrcoef(tX3[:,i],tX3[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,9,21,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX3=tX3[:,[i for i in range(tX3.shape[1]) if i not in [0,9,21,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Check correlation between regressors and the result value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_21=standartize(tX0)\n",
    "tX1_21=standartize(tX1)\n",
    "tX2_21=standartize(tX2)\n",
    "tX3_21=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_21..tX3_21 to y0...y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Don't the correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)\n",
    "\n",
    "#Replace NaNs with median\n",
    "tX0=np.copy(np.where(tX0==-999.0,mean0,tX0))\n",
    "tX1=np.copy(np.where(tX1==-999.0,mean1,tX1))\n",
    "tX2=np.copy(np.where(tX2==-999.0,mean2,tX2))\n",
    "tX3=np.copy(np.where(tX3==-999.0,mean3,tX3))\n",
    "\n",
    "#Data standartization\n",
    "tX0_22=standartize(tX0)\n",
    "tX1_22=standartize(tX1)\n",
    "tX2_22=standartize(tX2)\n",
    "tX3_22=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_22..tX3_22 to y0...y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
