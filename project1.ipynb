{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the only one column with integer values (it looks like clusters), so we divide the dataset by this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tX[:,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of clusters are:  (99913, 30) (77544, 30) (50379, 30) (22164, 30)\n"
     ]
    }
   ],
   "source": [
    "tX0=np.copy(tX[tX[:,22]==0,:])\n",
    "ids0=np.copy(ids[tX[:,22]==0])\n",
    "y0 = np.copy(y[tX[:,22]==0])\n",
    "\n",
    "tX1=np.copy(tX[tX[:,22]==1,:])\n",
    "ids1=np.copy(ids[tX[:,22]==1])\n",
    "y1 = np.copy(y[tX[:,22]==1])\n",
    "\n",
    "tX2=np.copy(tX[tX[:,22]==2,:])\n",
    "ids2=np.copy(ids[tX[:,22]==2])\n",
    "y2 =  np.copy(y[tX[:,22]==2])\n",
    "\n",
    "tX3=np.copy(tX[tX[:,22]==3,:])\n",
    "ids3=np.copy(ids[tX[:,22]==3])\n",
    "y3 =  np.copy(y[tX[:,22]==3])\n",
    "\n",
    "print('Shapes of clusters are: ',tX0.shape,tX1.shape,tX2.shape,tX3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting columns with clusters\n",
    "tX0=np.copy(np.delete(tX0,(22),axis=1))\n",
    "tX1=np.copy(np.delete(tX1,(22),axis=1))\n",
    "tX2=np.copy(np.delete(tX2,(22),axis=1))\n",
    "tX3=np.copy(np.delete(tX3,(22),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of NaNs in 0 cluster: \n",
      " [26123     0     0     0 99913 99913 99913     0     0     0     0     0\n",
      " 99913     0     0     0     0     0     0     0     0     0 99913 99913\n",
      " 99913 99913 99913 99913     0]\n",
      "Amount of NaNs in 1 cluster: \n",
      " [ 7562     0     0     0 77544 77544 77544     0     0     0     0     0\n",
      " 77544     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0 77544 77544 77544     0]\n",
      "Amount of NaNs in 2 cluster: \n",
      " [2952    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "Amount of NaNs in 3 cluster: \n",
      " [1477    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "#Counting NaNs in columns in each cluster\n",
    "print('Amount of NaNs in 0 cluster: \\n',np.count_nonzero(tX0==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 1 cluster: \\n',np.count_nonzero(tX1==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 2 cluster: \\n',np.count_nonzero(tX2==-999.0, axis = 0))\n",
    "print('Amount of NaNs in 3 cluster: \\n',np.count_nonzero(tX3==-999.0, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in clusters 0 and 1 some columns consist only of NaNs That's why we will delete these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting columns where all values are same\n",
    "tX0cl=np.copy(tX0[:,np.invert(np.all(tX0 == tX0[0,:], axis = 0))])\n",
    "tX1cl=np.copy(tX1[:,np.invert(np.all(tX1 == tX1[0,:], axis = 0))])\n",
    "tX2cl=np.copy(tX2[:,np.invert(np.all(tX2 == tX2[0,:], axis = 0))])\n",
    "tX3cl=np.copy(tX3[:,np.invert(np.all(tX3 == tX3[0,:], axis = 0))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have cleaned all columns except the first one. We have different ways to work with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Delete rows with NaNs in the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting rows with NaNs\n",
    "ids0d=np.copy(ids0[tX0[:,0]!=-999.0])\n",
    "y0d = np.copy(y0[tX0[:,0]!=-999.0])\n",
    "tX0=np.copy(tX0[tX0[:,0]!=-999.0,:])\n",
    "\n",
    "ids1d=np.copy(ids1[tX1[:,0]!=-999.0])\n",
    "y1d = np.copy(y1[tX1[:,0]!=-999.0])\n",
    "tX1=np.copy(tX1[tX1[:,0]!=-999.0,:])\n",
    "\n",
    "ids2d=np.copy(ids2[tX2[:,0]!=-999.0])\n",
    "y2d = np.copy(y2[tX2[:,0]!=-999.0])\n",
    "tX2=np.copy(tX2[tX2[:,0]!=-999.0,:])\n",
    "\n",
    "ids3d=np.copy(ids3[tX3[:,0]!=-999.0])\n",
    "y3d = np.copy(y3[tX3[:,0]!=-999.0])\n",
    "tX3=np.copy(tX3[tX3[:,0]!=-999.0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. See the correlation between regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.9428649994377921\n",
      "Pearson correlation between column 1 and 7 : 0.7025329814432556\n",
      "Pearson correlation between column 3 and 5 : 0.9999999999991431\n",
      "Pearson correlation between column 6 and 9 : 0.7921536842510372\n",
      "Pearson correlation between column 6 and 12 : 0.7801737531532945\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX0\n",
    "for i in range(len(tX0[0,:])):\n",
    "    for j in range(i+1,len(tX0[0,:])):\n",
    "        corr = np.corrcoef(tX0[:,i],tX0[:,j])\n",
    "        if np.abs(np.corrcoef(tX0[:,i],tX0[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it makes sense to drop columns 0,3,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX0=tX0[:,[i for i in range(tX0.shape[1]) if i not in [0,3,6]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.9203955805006697\n",
      "Pearson correlation between column 3 and 6 : 0.880515673723393\n",
      "Pearson correlation between column 3 and 15 : 0.7067535021056447\n",
      "Pearson correlation between column 3 and 17 : 0.719783491337637\n",
      "Pearson correlation between column 3 and 18 : 0.9473952774313399\n",
      "Pearson correlation between column 3 and 21 : 0.9473952764775125\n",
      "Pearson correlation between column 6 and 17 : 0.7682569234879949\n",
      "Pearson correlation between column 6 and 18 : 0.9128480282626462\n",
      "Pearson correlation between column 6 and 21 : 0.9128480302418414\n",
      "Pearson correlation between column 7 and 12 : 0.7106467382693462\n",
      "Pearson correlation between column 17 and 18 : 0.721078263561643\n",
      "Pearson correlation between column 17 and 21 : 0.7210782626271482\n",
      "Pearson correlation between column 18 and 21 : 0.9999999999990334\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX1\n",
    "for i in range(len(tX1[0,:])):\n",
    "    for j in range(i+1,len(tX1[0,:])):\n",
    "        corr = np.corrcoef(tX1[:,i],tX1[:,j])\n",
    "        if np.abs(np.corrcoef(tX1[:,i],tX1[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop columns 0,3,6,21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX1=tX1[:,[i for i in range(tX1.shape[1]) if i not in [0,3,6,21]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8882697562122746\n",
      "Pearson correlation between column 3 and 9 : 0.8100583553617612\n",
      "Pearson correlation between column 3 and 19 : 0.7727347641970795\n",
      "Pearson correlation between column 3 and 21 : 0.7106329242566304\n",
      "Pearson correlation between column 3 and 22 : 0.8064082142710389\n",
      "Pearson correlation between column 3 and 28 : 0.7651054026528024\n",
      "Pearson correlation between column 4 and 5 : 0.8102770843828327\n",
      "Pearson correlation between column 4 and 6 : -0.8533511261674986\n",
      "Pearson correlation between column 5 and 6 : -0.7806386730593765\n",
      "Pearson correlation between column 9 and 21 : 0.8630680375230015\n",
      "Pearson correlation between column 9 and 22 : 0.9212004546370948\n",
      "Pearson correlation between column 9 and 28 : 0.9417446400854351\n",
      "Pearson correlation between column 10 and 16 : 0.7427175522749608\n",
      "Pearson correlation between column 21 and 22 : 0.8087380200255875\n",
      "Pearson correlation between column 21 and 28 : 0.8269810542575168\n",
      "Pearson correlation between column 22 and 28 : 0.9584434603145308\n",
      "Pearson correlation between column 25 and 28 : 0.7179977621623423\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX2\n",
    "for i in range(len(tX2[0,:])):\n",
    "    for j in range(i+1,len(tX2[0,:])):\n",
    "        corr = np.corrcoef(tX2[:,i],tX2[:,j])\n",
    "        if np.abs(np.corrcoef(tX2[:,i],tX2[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,3,4,9,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX2=tX2[:,[i for i in range(tX2.shape[1]) if i not in [0,3,4,9,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8939715045042343\n",
      "Pearson correlation between column 3 and 19 : 0.7852631621337898\n",
      "Pearson correlation between column 4 and 5 : 0.758652958037284\n",
      "Pearson correlation between column 4 and 6 : -0.7768024192719148\n",
      "Pearson correlation between column 9 and 21 : 0.9209360084498386\n",
      "Pearson correlation between column 9 and 22 : 0.8823253774045843\n",
      "Pearson correlation between column 9 and 25 : 0.7709015614616341\n",
      "Pearson correlation between column 9 and 28 : 0.957878606826293\n",
      "Pearson correlation between column 10 and 16 : 0.7636779915431535\n",
      "Pearson correlation between column 21 and 22 : 0.817603943920867\n",
      "Pearson correlation between column 21 and 25 : 0.7259662973246287\n",
      "Pearson correlation between column 21 and 28 : 0.8968125698294412\n",
      "Pearson correlation between column 22 and 28 : 0.8874163028182186\n",
      "Pearson correlation between column 25 and 28 : 0.83669471775916\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX3\n",
    "for i in range(len(tX3[0,:])):\n",
    "    for j in range(i+1,len(tX3[0,:])):\n",
    "        corr = np.corrcoef(tX3[:,i],tX3[:,j])\n",
    "        if np.abs(np.corrcoef(tX3[:,i],tX3[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,9,21,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX3=tX3[:,[i for i in range(tX3.shape[1]) if i not in [0,9,21,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Check correlation between regressors and the result value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_11=standartize(tX0)\n",
    "tX1_11=standartize(tX1)\n",
    "tX2_11=standartize(tX2)\n",
    "tX3_11=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_11..tX3_11 to y0d...y3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-2.06880312e-01, -4.58597236e-01,  3.52838215e-01,  3.23421071e-02,\n",
       "         -1.58959755e-01, -1.15919349e-02,  2.81667771e-01,  9.47439000e-05,\n",
       "          4.26477717e-04,  1.65336871e-01,  4.68799168e-03,  6.38468391e-04,\n",
       "         -2.37790473e-02, -6.95609668e-03,  3.95805980e-02]),\n",
       "  array([-2.06301728e-01, -2.50211556e-01,  2.76199667e-01,  3.96837306e-02,\n",
       "         -1.91784626e-01,  1.18899478e-01,  1.77286403e-01,  3.66955178e-03,\n",
       "         -6.06497299e-03,  2.48889753e-01, -8.70445912e-03, -8.68657328e-07,\n",
       "          4.95246061e-02,  4.34505291e-03, -2.81561347e-02,  5.48007959e-02,\n",
       "         -5.30974371e-04, -2.24981539e-03]),\n",
       "  array([-0.16371398, -0.15131667,  0.20788056,  0.00735713,  0.22116094,\n",
       "         -0.0555752 , -0.15740836,  0.13985692,  0.18505956,  0.21305616,\n",
       "         -0.00308169,  0.00068213,  0.2712701 , -0.00727277,  0.00293023,\n",
       "          0.15992448,  0.0052238 , -0.11861323,  0.00838199,  0.00867409,\n",
       "          0.0940448 ,  0.0048366 , -0.00577548, -0.1505115 ]),\n",
       "  array([-1.03408034e-01, -1.38354123e-01,  2.54979444e-01, -1.10458663e-01,\n",
       "          2.14256921e-01,  2.75855932e-02,  2.33258219e-01,  1.52286487e-02,\n",
       "         -1.74817676e-01,  7.64653969e-02,  1.02163275e-01,  7.07684288e-02,\n",
       "         -8.72203636e-03, -2.82541050e-04,  1.61474625e-01, -4.10706389e-03,\n",
       "          3.86116034e-03,  1.60786541e-02,  5.90055502e-03, -2.42244334e-03,\n",
       "         -1.32060349e-03,  3.48126383e-02,  8.27382761e-03,  1.36851344e-03,\n",
       "         -2.92132137e-01])],\n",
       " [0.8655253268669649,\n",
       "  0.9146404549646151,\n",
       "  0.8531872782441865,\n",
       "  0.9358064809053154])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [y0d, y1d, y2d, y3d]\n",
    "tXst = [tX0_11, tX1_11, tX2_11, tX3_11]\n",
    "def LinearRegressionSubmission(y, tXst):\n",
    "    w_list = []\n",
    "    rmse_list = []\n",
    "    for i in range(len(y)):\n",
    "        w, mse = least_squares(y[i], tXst[i])\n",
    "        w_list.append(w)\n",
    "        rmse_list.append(np.sqrt(2*mse))\n",
    "    return w_list, rmse_list\n",
    "LinearRegressionSubmission(y, tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear regression with polynomial expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(y, tXst):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "    \n",
    "    # define the structure of the figure\n",
    "    num_row = 2\n",
    "    num_col = 2\n",
    "    f, axs = plt.subplots(num_row, num_col)\n",
    "    mean_rmse = []\n",
    "    rmse_list = []\n",
    "    min_error = 1000\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        w0, mse0 = least_squares(y[0], build_poly(tXst[0],degree))\n",
    "        w1, mse1 = least_squares(y[1], build_poly(tXst[1],degree))\n",
    "        w2, mse2 = least_squares(y[2], build_poly(tXst[2],degree))\n",
    "        w3, mse3 = least_squares(y[3], build_poly(tXst[3],degree))\n",
    "        err= np.array([mse0,mse1,mse2,mse3])\n",
    "        rmse_list = []\n",
    "        for i in range(len(err)):\n",
    "            rmse_list.append(np.sqrt(2*err[i]))\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse_list))\n",
    "        \n",
    "        #to store the weights with the minimum training error\n",
    "        if np.mean(rmse_list) < min_error:\n",
    "            min_error = np.mean(rmse_list)\n",
    "            w0f,w1f,w2f,w3f = w0,w1,w2,w3\n",
    "\n",
    "        mean_rmse.append(np.mean(rmse_list))\n",
    "        # plot fit\n",
    "        #plot_fitted_curve(y, x, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(\"visualize_polynomial_regression\")\n",
    "    #plt.show()\n",
    "    print(\"Best degree: {degree}, training error: {err}\".format(degree=mean_rmse.index(min(mean_rmse)),err=np.min(mean_rmse)))\n",
    "    return degree, np.min(mean_rmse),[w0f,w1f,w2f,w3f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=[0.7910167024107171, 0.8857077259821294, 0.8506684510165388, 0.8640785210235167]\n",
      "Processing 2th experiment, degree=3, rmse=[0.7620364562247038, 0.8351179266767852, 0.7981937795351055, 0.8093542931845197]\n",
      "Processing 3th experiment, degree=7, rmse=[0.7506363097771088, 0.8189092402139859, 0.7709679140699045, 0.794044608684681]\n",
      "Processing 4th experiment, degree=12, rmse=[0.7467773789812082, 0.8005811377963961, 0.7551286163235877, 0.7694411157031958]\n",
      "Best degree: 3, training error: 0.767982062201097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-f4b505f0879c>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prevision0=(np.array([ids[0],ypred_list[0]]).T)\n",
      "<ipython-input-23-f4b505f0879c>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prevision1=(np.array([ids[1],ypred_list[1]]).T)\n",
      "<ipython-input-23-f4b505f0879c>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prevision2=(np.array([ids[2],ypred_list[2]]).T)\n",
      "<ipython-input-23-f4b505f0879c>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prevision3=(np.array([ids[3],ypred_list[3]]).T)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f4b505f0879c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprevision\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "degree,_,w = polynomial_regression(y,tXst)\n",
    "ypred_list = []\n",
    "rmse_list = []\n",
    "sub = []\n",
    "for i in range(4):\n",
    "    ypred = predict_labels(w[i], build_poly(tXst[i],degree))\n",
    "    ypred_list.append(ypred)\n",
    "\n",
    "#creating submission format\n",
    "prevision0=(np.array([ids[0],ypred_list[0]]).T)\n",
    "prevision1=(np.array([ids[1],ypred_list[1]]).T)\n",
    "prevision2=(np.array([ids[2],ypred_list[2]]).T)\n",
    "prevision3=(np.array([ids[3],ypred_list[3]]).T)\n",
    "prevision = np.concatenate((prevision0,prevision1,prevision2,prevision3), axis=0)\n",
    "\n",
    "\n",
    "submission = sorted(prevision, key=prevision[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescentSubmission(y, tXst):\n",
    "  # Define the parameters of the algorithm.\n",
    "    max_iters = 100\n",
    "    gamma = 0.5\n",
    "  # Initialization\n",
    "    \n",
    "    w_list = []\n",
    "    loss_list = []\n",
    "  # Start GD\n",
    "    for i in range(4):\n",
    "        w_initial = np.array([0 for i in range(tXst[i].shape[1])])\n",
    "        gradient_loss, gradient_w = gradient_descent(y[i], tXst[i], w_initial, max_iters, gamma)\n",
    "        w_list.append(gradient_w)\n",
    "        loss_list.append(gradient_loss)\n",
    "    return w_list, loss_list\n",
    "GradientDescentSubmission(y,tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-0.03953648, -0.00835486,  0.01807197, -0.00663377, -0.02433187,\n",
       "          0.01337897,  0.01250804,  0.00348614, -0.00726161, -0.01795264,\n",
       "         -0.0077468 ,  0.01124808, -0.02361948, -0.01893504,  0.00024943]),\n",
       "  array([-0.02351591,  0.00108344,  0.01301907, -0.00146986, -0.03370089,\n",
       "          0.02917919,  0.02215244, -0.00269818, -0.01068807, -0.01748665,\n",
       "         -0.00024932, -0.00090134,  0.00666417, -0.00540354,  0.00534665,\n",
       "          0.01400693, -0.00442097,  0.0080783 ]),\n",
       "  array([-0.01726204, -0.00730088,  0.03067101, -0.0321546 , -0.00369207,\n",
       "         -0.00240034, -0.02041713,  0.00798676,  0.03914496,  0.02928135,\n",
       "          0.00298133, -0.00496922, -0.00728747, -0.00343598,  0.01795668,\n",
       "         -0.00532421, -0.00087185, -0.00793809, -0.00393357, -0.02118535,\n",
       "         -0.00500613,  0.0070464 ,  0.00690569, -0.00915552]),\n",
       "  array([-0.01108834,  0.00796019,  0.02183295, -0.00056793, -0.00041752,\n",
       "         -0.00297566, -0.00638126,  0.00488044, -0.00836563,  0.02253265,\n",
       "          0.01397209,  0.01145584,  0.00063767,  0.00661705,  0.00477859,\n",
       "         -0.00297739, -0.00743506,  0.01387951,  0.00011055, -0.00834455,\n",
       "         -0.00597571, -0.00819478,  0.00544559, -0.00489284,  0.00141563])],\n",
       " [0.4651956435121739,\n",
       "  0.474697531957048,\n",
       "  0.45932730119435167,\n",
       "  0.48683941341480225])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def StochasticGradientDescent(y, tXst):\n",
    "  # Define the parameters of the algorithm.\n",
    "    max_iters = 100\n",
    "    gamma = 0.001\n",
    "    batch_size = 1\n",
    "  # Initialization\n",
    "    w_list = []\n",
    "    loss_list = []\n",
    "  # Start SGD.\n",
    "    for i in range(4):\n",
    "        w_initial = np.array([0 for i in range(tXst[i].shape[1])])\n",
    "        sgd_losses, sgd_w = stochastic_gradient_descent(y[i], tXst[i], w_initial, batch_size, max_iters, gamma)\n",
    "        w_list.append(sgd_w)\n",
    "        loss_list.append(sgd_losses)\n",
    "    return w_list, loss_list\n",
    "StochasticGradientDescent(y, tXst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for 0 subset are: 1 0.01  with error:  0.7929677144488401\n",
      "Best parameters for 1 subset are: 2 0.01  with error:  0.8450561850194014\n",
      "Best parameters for 2 subset are: 2 0.01  with error:  0.8173424201564052\n",
      "Best parameters for 3 subset are: 2 0.01  with error:  0.8320016918872625\n",
      "[[0.93630976 0.93645421 0.93732362 0.94137645 0.95317962 0.97125258\n",
      "  0.98617216 0.99424679]\n",
      " [0.79296771 0.7959668  0.80461279 0.82188364 0.85171029 0.89660549\n",
      "  0.94290872 0.97407404]\n",
      " [2.63784812 2.50596282 2.11248506 1.4968839  0.92503176 0.94594117\n",
      "  1.17291633 1.16651706]] [[0.97364297 0.97370116 0.9740531  0.97569909 0.98052021 0.98797365\n",
      "  0.99418946 0.99757674]\n",
      " [0.88634788 0.88770695 0.89194932 0.90119727 0.91843812 0.94433812\n",
      "  0.96988659 0.98647535]\n",
      " [0.84505619 0.84631685 0.84932718 0.85628768 0.8726649  0.90067554\n",
      "  0.93105463 0.95601466]] [[0.99785752 0.99786204 0.99788998 0.9980218  0.99841024 0.99901492\n",
      "  0.99952264 0.9998006 ]\n",
      " [0.85156878 0.85317203 0.85792353 0.86728612 0.88394989 0.91290307\n",
      "  0.94846495 0.97555061]\n",
      " [0.81734242 0.81842084 0.82230268 0.83287073 0.85398572 0.88540182\n",
      "  0.92024966 0.95092966]] [[0.93321668 0.93336927 0.93428432 0.93854342 0.95093514 0.96988973\n",
      "  0.98552227 0.99397763]\n",
      " [0.86587244 0.86764829 0.87254226 0.88319215 0.90506429 0.93751498\n",
      "  0.96714593 0.98541129]\n",
      " [0.83200169 0.83433777 0.83868516 0.8473625  0.86204602 0.88072703\n",
      "  0.90335707 0.93007632]]\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-2, 1, 8)\n",
    "    # split data in k fold\n",
    "    num_degrees=3\n",
    "    # define lists to store the loss of training data and test data\n",
    "    #matrices for storing lists (for 4 subsets) of errors\n",
    "    #matrix_te=np.zeros(shape=(num_degrees,len(lambdas)),dtype=list)\n",
    "    #matrix_tr=np.zeros(shape=(num_degrees,len(lambdas)),dtype=list)\n",
    "    matrix_te0=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr0=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te1=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr1=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te2=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr2=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_te3=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    matrix_tr3=np.zeros(shape=(num_degrees,len(lambdas)))\n",
    "    for degree in np.arange(num_degrees):\n",
    "        for ind_lmbd,lambda_ in enumerate(lambdas):\n",
    "            #losses_te=[]\n",
    "            #losses_tr=[]\n",
    "            for i in range(0,4):\n",
    "                k_indices,indices = build_k_indices(y[i], k_fold, seed)\n",
    "                loss_tr=0\n",
    "                loss_te=0\n",
    "                for k in range(k_fold):\n",
    "                    #print([degree,ind_lmbd,i,k])\n",
    "                    l_tr, l_te,_ = cross_validation_ridge(y[i], tXst[i], k_indices, k, lambda_, degree)\n",
    "                    loss_tr+=l_tr\n",
    "                    loss_te+=l_te\n",
    "                if i==0:\n",
    "                    matrix_te0[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr0[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==1:\n",
    "                    matrix_te1[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr1[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==2:\n",
    "                    matrix_te2[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr2[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                elif i==3:\n",
    "                    matrix_te3[degree][ind_lmbd]=loss_te/k_fold\n",
    "                    matrix_tr3[degree][ind_lmbd]=loss_tr/k_fold\n",
    "                #losses_te.append(np.mean(loss_te))\n",
    "                #losses_tr.append(np.mean(loss_tr))\n",
    "            #matrix_tr[degree][ind_lmbd]= losses_tr\n",
    "            #matrix_te[degree][ind_lmbd]=losses_te\n",
    "    \n",
    "   \n",
    "    \n",
    "    #get the best degree lambda couple  \n",
    "    result = np.where(matrix_te0 == np.amin(matrix_te0))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 0 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te0[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te1 == np.amin(matrix_te1))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 1 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te1[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te2 == np.amin(matrix_te2))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 2 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te2[best_degree][best_lambda])\n",
    "    \n",
    "    result = np.where(matrix_te3 == np.amin(matrix_te3))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    best_degree = listOfCoordinates[0][0]\n",
    "    best_lambda = listOfCoordinates[0][1]\n",
    "    print('Best parameters for 3 subset are:',best_degree,lambdas[best_lambda],' with error: ',matrix_te3[best_degree][best_lambda])\n",
    "    \n",
    "    print(matrix_te0,matrix_te1,matrix_te2,matrix_te3)\n",
    "    #show cross validation for each degree\n",
    "    \"\"\"\n",
    "    ind_row = 0\n",
    "    for row in matrix_te:\n",
    "        degree = ind_row\n",
    "        lambdas = list(range(len(row)))\n",
    "        rmse_te = row\n",
    "        rmse_tr = matrix_tr[ind_row, :]\n",
    "        cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "        ind_row += 1\n",
    "    plt.show()\n",
    "\n",
    "    return best_degree, best_lambda\n",
    "    \"\"\"\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    t=np.copy(x)\n",
    "    t[t>0]=1/(1+np.exp(-t[t>0]))\n",
    "    t[t<0]=np.exp(t[t<0])/(1+np.exp(t[t<0]))\n",
    "    return t\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss=((y+1)/2.0).T.dot(np.log(pred+1e-20)) + (1 - ((y+1)/2.0)).T.dot(np.log(1 - pred+1e-20))\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred-(y+1)/2.0)\n",
    "    return grad\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    loss=calculate_loss(y,tx,w)\n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    w = w-gamma*grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-6\n",
    "    gamma = 0.00001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    #w = np.random.rand(tx.shape[1], 1)\n",
    "    w=np.zeros((tx.shape[1],1))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        #print(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-6\n",
    "    gamma = 0.00001\n",
    "    all_losses=[]\n",
    "    weights=[]\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        losses = []\n",
    "        \n",
    "        # build tx and ty\n",
    "        tx = np.c_[np.ones((y[i].shape[0], 1)), x[i]]\n",
    "        w = np.random.rand(tx.shape[1], 1)\n",
    "        ty=y[i].reshape(y[i].shape[0],1)\n",
    "\n",
    "        print('Training for subset {0}\\n'.format(i))\n",
    "        # start the logistic regression\n",
    "        for iter in range(max_iter):\n",
    "            \n",
    "            # get loss and update w\n",
    "            loss, w = learning_by_gradient_descent(ty, tx, w, gamma)\n",
    "            \n",
    "            # log info\n",
    "            if iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "            \n",
    "            #store loss\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # converge criterion\n",
    "            if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "                break\n",
    "\n",
    "        print(\"\\nBest loss for subset {i}={l}\\n\".format(i=i,l=calculate_loss(ty, tx, w)))\n",
    "        all_losses.append(losses)\n",
    "        weights.append(w)\n",
    "    return all_losses,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for subset 0\n",
      "\n",
      "Current iteration=0, loss=-114597.1840925516\n",
      "Current iteration=100, loss=-33872.57395556392\n",
      "Current iteration=200, loss=-33476.195047984336\n",
      "Current iteration=300, loss=-33343.64827424953\n",
      "Current iteration=400, loss=-33292.071286279366\n",
      "Current iteration=500, loss=-33270.29552543046\n",
      "Current iteration=600, loss=-33260.62300141582\n",
      "Current iteration=700, loss=-33256.17733863803\n",
      "Current iteration=800, loss=-33254.08412781131\n",
      "Current iteration=900, loss=-33253.080985782806\n",
      "Current iteration=1000, loss=-33252.59381657056\n",
      "Current iteration=1100, loss=-33252.35480512532\n",
      "Current iteration=1200, loss=-33252.23661105214\n",
      "Current iteration=1300, loss=-33252.17779797679\n",
      "Current iteration=1400, loss=-33252.148388331676\n",
      "Current iteration=1500, loss=-33252.13362426477\n",
      "Current iteration=1600, loss=-33252.12618929323\n",
      "Current iteration=1700, loss=-33252.1224357776\n",
      "Current iteration=1800, loss=-33252.120537031544\n",
      "Current iteration=1900, loss=-33252.11957499187\n",
      "Current iteration=2000, loss=-33252.119086925726\n",
      "Current iteration=2100, loss=-33252.118839061666\n",
      "\n",
      "Best loss for subset 0=-33252.11872896005\n",
      "\n",
      "Training for subset 1\n",
      "\n",
      "Current iteration=0, loss=-86549.57663949048\n",
      "Current iteration=100, loss=-39767.2736850546\n",
      "Current iteration=200, loss=-39650.834773825285\n",
      "Current iteration=300, loss=-39632.22499579188\n",
      "Current iteration=400, loss=-39628.24286453081\n",
      "Current iteration=500, loss=-39627.31579193864\n",
      "Current iteration=600, loss=-39627.09401803938\n",
      "Current iteration=700, loss=-39627.04044703559\n",
      "Current iteration=800, loss=-39627.027457407516\n",
      "Current iteration=900, loss=-39627.024302759535\n",
      "Current iteration=1000, loss=-39627.02353609306\n",
      "\n",
      "Best loss for subset 1=-39627.02335880786\n",
      "\n",
      "Training for subset 2\n",
      "\n",
      "Current iteration=0, loss=-69218.73174763971\n",
      "Current iteration=100, loss=-25210.10004430102\n",
      "Current iteration=200, loss=-25031.478395015823\n",
      "Current iteration=300, loss=-24993.13892379336\n",
      "Current iteration=400, loss=-24982.775151858248\n",
      "Current iteration=500, loss=-24979.80721980621\n",
      "Current iteration=600, loss=-24978.935030679164\n",
      "Current iteration=700, loss=-24978.674932852897\n",
      "Current iteration=800, loss=-24978.596648320665\n",
      "Current iteration=900, loss=-24978.572938228626\n",
      "Current iteration=1000, loss=-24978.565724738226\n",
      "Current iteration=1100, loss=-24978.563522643883\n",
      "Current iteration=1200, loss=-24978.562848598453\n",
      "Current iteration=1300, loss=-24978.56264183006\n",
      "\n",
      "Best loss for subset 2=-24978.562632590652\n",
      "\n",
      "Training for subset 3\n",
      "\n",
      "Current iteration=0, loss=-30213.067943260336\n",
      "Current iteration=100, loss=-11503.409265784157\n",
      "Current iteration=200, loss=-11388.364702551626\n",
      "Current iteration=300, loss=-11342.460040887201\n",
      "Current iteration=400, loss=-11318.417638749263\n",
      "Current iteration=500, loss=-11305.083176014701\n",
      "Current iteration=600, loss=-11297.332792901612\n",
      "Current iteration=700, loss=-11292.627343201995\n",
      "Current iteration=800, loss=-11289.649810751365\n",
      "Current iteration=900, loss=-11287.69057088569\n",
      "Current iteration=1000, loss=-11286.354044211217\n",
      "Current iteration=1100, loss=-11285.412516705135\n",
      "Current iteration=1200, loss=-11284.730668098007\n",
      "Current iteration=1300, loss=-11284.225436361758\n",
      "Current iteration=1400, loss=-11283.844120431473\n",
      "Current iteration=1500, loss=-11283.552151553205\n",
      "Current iteration=1600, loss=-11283.326108419158\n",
      "Current iteration=1700, loss=-11283.149634603806\n",
      "Current iteration=1800, loss=-11283.010993393145\n",
      "Current iteration=1900, loss=-11282.901564248876\n",
      "Current iteration=2000, loss=-11282.814891819888\n",
      "Current iteration=2100, loss=-11282.746066377045\n",
      "Current iteration=2200, loss=-11282.691307897734\n",
      "Current iteration=2300, loss=-11282.647678630648\n",
      "Current iteration=2400, loss=-11282.612879028344\n",
      "Current iteration=2500, loss=-11282.58509936239\n",
      "Current iteration=2600, loss=-11282.562909605298\n",
      "Current iteration=2700, loss=-11282.545176326093\n",
      "Current iteration=2800, loss=-11282.530999122522\n",
      "Current iteration=2900, loss=-11282.519661480706\n",
      "Current iteration=3000, loss=-11282.510592476752\n",
      "Current iteration=3100, loss=-11282.503336741634\n",
      "Current iteration=3200, loss=-11282.497530794859\n",
      "Current iteration=3300, loss=-11282.49288432961\n",
      "Current iteration=3400, loss=-11282.48916537331\n",
      "Current iteration=3500, loss=-11282.48618849686\n",
      "Current iteration=3600, loss=-11282.483805431555\n",
      "Current iteration=3700, loss=-11282.481897593047\n",
      "Current iteration=3800, loss=-11282.480370119167\n",
      "Current iteration=3900, loss=-11282.479147111648\n",
      "Current iteration=4000, loss=-11282.478167836398\n",
      "Current iteration=4100, loss=-11282.47738368788\n",
      "Current iteration=4200, loss=-11282.476755763004\n",
      "Current iteration=4300, loss=-11282.476252921559\n",
      "Current iteration=4400, loss=-11282.475850235227\n",
      "Current iteration=4500, loss=-11282.475527747065\n",
      "Current iteration=4600, loss=-11282.475269479106\n",
      "Current iteration=4700, loss=-11282.475062638372\n",
      "Current iteration=4800, loss=-11282.474896981497\n",
      "Current iteration=4900, loss=-11282.47476430626\n",
      "\n",
      "Best loss for subset 3=-11282.47467944153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss,w=logistic_regression_gradient_descent_demo(y,tXst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for subset 0 is 0.7878845372001626\n",
      "Accuracy for subset 1 is 0.6924494870109457\n",
      "Accuracy for subset 2 is 0.7306175807029751\n",
      "Accuracy for subset 3 is 0.7179871416831827\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "for i in range(4):\n",
    "    print('Accuracy for subset {i} is {acc}'.format(i=i,acc=np.sum(np.where(sigmoid(np.c_[np.ones((y[i].shape[0], 1)), tXst[i]]@w[i])>0.5,1,0)==((y[i].reshape(y[i].shape[0],1)+1)/2.0).astype('int'))/y[i].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Don't see the correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_12=standartize(tX0cl)\n",
    "tX1_12=standartize(tX1cl)\n",
    "tX2_12=standartize(tX2cl)\n",
    "tX3_12=standartize(tX3cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_12..tX3_12 to y0d...y3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Substitute rows with NaNs in the first column by the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean for 1st column without -999.0 values\n",
    "mean0=tX0[tX0[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean1=tX1[tX1[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean2=tX2[tX2[:,0]!=-999.0].mean(axis=0)[0]\n",
    "mean3=tX3[tX3[:,0]!=-999.0].mean(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX0[tX0[:,0]!=-999.0].mean(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX0=np.copy(np.where(tX0==-999.0,mean0,tX0))\n",
    "tX1=np.copy(np.where(tX1==-999.0,mean1,tX1))\n",
    "tX2=np.copy(np.where(tX2==-999.0,mean2,tX2))\n",
    "tX3=np.copy(np.where(tX3==-999.0,mean3,tX3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. See the correlation between regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.7488162009965309\n",
      "Pearson correlation between column 3 and 5 : 0.9999999999988465\n",
      "Pearson correlation between column 6 and 9 : 0.8002688533328596\n",
      "Pearson correlation between column 6 and 12 : 0.7797439568172713\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX0\n",
    "for i in range(len(tX0[0,:])):\n",
    "    for j in range(i+1,len(tX0[0,:])):\n",
    "        corr = np.corrcoef(tX0[:,i],tX0[:,j])\n",
    "        if np.abs(np.corrcoef(tX0[:,i],tX0[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it makes sense to drop columns 0,3,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX0=tX0[:,[i for i in range(tX0.shape[1]) if i not in [0,3,6]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8445009835185863\n",
      "Pearson correlation between column 3 and 6 : 0.8632110677076686\n",
      "Pearson correlation between column 3 and 17 : 0.7125818159025532\n",
      "Pearson correlation between column 3 and 18 : 0.9367590332144766\n",
      "Pearson correlation between column 3 and 21 : 0.9367590311733165\n",
      "Pearson correlation between column 6 and 17 : 0.757990128208487\n",
      "Pearson correlation between column 6 and 18 : 0.9051662981927852\n",
      "Pearson correlation between column 6 and 21 : 0.905166300382275\n",
      "Pearson correlation between column 7 and 12 : 0.709867998636906\n",
      "Pearson correlation between column 17 and 18 : 0.7068771933564622\n",
      "Pearson correlation between column 17 and 21 : 0.7068771905369512\n",
      "Pearson correlation between column 18 and 21 : 0.999999999999017\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX1\n",
    "for i in range(len(tX1[0,:])):\n",
    "    for j in range(i+1,len(tX1[0,:])):\n",
    "        corr = np.corrcoef(tX1[:,i],tX1[:,j])\n",
    "        if np.abs(np.corrcoef(tX1[:,i],tX1[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop columns 0,3,6,21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX1=tX1[:,[i for i in range(tX1.shape[1]) if i not in [0,3,6,21]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8228939521810881\n",
      "Pearson correlation between column 3 and 9 : 0.800559238241552\n",
      "Pearson correlation between column 3 and 19 : 0.7285433946032545\n",
      "Pearson correlation between column 3 and 21 : 0.7069249354905159\n",
      "Pearson correlation between column 3 and 22 : 0.7995787774292449\n",
      "Pearson correlation between column 3 and 28 : 0.7584645222360212\n",
      "Pearson correlation between column 4 and 5 : 0.8098715851425442\n",
      "Pearson correlation between column 4 and 6 : -0.8501315483608742\n",
      "Pearson correlation between column 5 and 6 : -0.7780946604694609\n",
      "Pearson correlation between column 9 and 21 : 0.8523147287795626\n",
      "Pearson correlation between column 9 and 22 : 0.9191164357409889\n",
      "Pearson correlation between column 9 and 28 : 0.9397870414625126\n",
      "Pearson correlation between column 10 and 16 : 0.7438565749365871\n",
      "Pearson correlation between column 21 and 22 : 0.7970772581576845\n",
      "Pearson correlation between column 21 and 28 : 0.8147451242277628\n",
      "Pearson correlation between column 22 and 28 : 0.9584459537718197\n",
      "Pearson correlation between column 25 and 28 : 0.720740253151538\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX2\n",
    "for i in range(len(tX2[0,:])):\n",
    "    for j in range(i+1,len(tX2[0,:])):\n",
    "        corr = np.corrcoef(tX2[:,i],tX2[:,j])\n",
    "        if np.abs(np.corrcoef(tX2[:,i],tX2[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,3,4,9,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX2=tX2[:,[i for i in range(tX2.shape[1]) if i not in [0,3,4,9,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between column 0 and 2 : 0.8172288262177909\n",
      "Pearson correlation between column 3 and 19 : 0.7502599406025436\n",
      "Pearson correlation between column 4 and 5 : 0.758966330984028\n",
      "Pearson correlation between column 4 and 6 : -0.7735086978837818\n",
      "Pearson correlation between column 9 and 21 : 0.9160946827155181\n",
      "Pearson correlation between column 9 and 22 : 0.8813251894172176\n",
      "Pearson correlation between column 9 and 25 : 0.7708057103889608\n",
      "Pearson correlation between column 9 and 28 : 0.9569498612168339\n",
      "Pearson correlation between column 10 and 16 : 0.7683893227784884\n",
      "Pearson correlation between column 21 and 22 : 0.8132272408011297\n",
      "Pearson correlation between column 21 and 25 : 0.7219329222297065\n",
      "Pearson correlation between column 21 and 28 : 0.892063808128829\n",
      "Pearson correlation between column 22 and 28 : 0.887903038587193\n",
      "Pearson correlation between column 25 and 28 : 0.8365333266992916\n"
     ]
    }
   ],
   "source": [
    "#Correlation between regressors in tX3\n",
    "for i in range(len(tX3[0,:])):\n",
    "    for j in range(i+1,len(tX3[0,:])):\n",
    "        corr = np.corrcoef(tX3[:,i],tX3[:,j])\n",
    "        if np.abs(np.corrcoef(tX3[:,i],tX3[:,j])[0,1])> 0.7:\n",
    "            print(\"Pearson correlation between column {i} and {j} : {corr}\".format(i=i,j=j,corr=corr[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 0,9,21,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "tX3=tX3[:,[i for i in range(tX3.shape[1]) if i not in [0,9,21,22]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Check correlation between regressors and the result value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standartization\n",
    "tX0_21=standartize(tX0)\n",
    "tX1_21=standartize(tX1)\n",
    "tX2_21=standartize(tX2)\n",
    "tX3_21=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_21..tX3_21 to y0...y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Don't the correlation between regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't overwrite global variables\n",
    "tX0=np.copy(tX0cl)\n",
    "tX1=np.copy(tX1cl)\n",
    "tX2=np.copy(tX2cl)\n",
    "tX3=np.copy(tX3cl)\n",
    "\n",
    "#Replace NaNs with median\n",
    "tX0=np.copy(np.where(tX0==-999.0,mean0,tX0))\n",
    "tX1=np.copy(np.where(tX1==-999.0,mean1,tX1))\n",
    "tX2=np.copy(np.where(tX2==-999.0,mean2,tX2))\n",
    "tX3=np.copy(np.where(tX3==-999.0,mean3,tX3))\n",
    "\n",
    "#Data standartization\n",
    "tX0_22=standartize(tX0)\n",
    "tX1_22=standartize(tX1)\n",
    "tX2_22=standartize(tX2)\n",
    "tX3_22=standartize(tX3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation (tX0_22..tX3_22 to y0...y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
